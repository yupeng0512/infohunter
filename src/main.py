"""InfoHunter ä¸»è°ƒåº¦å™¨

å¤šæºç¤¾äº¤åª’ä½“ AI æ™ºèƒ½è®¢é˜…ç›‘æ§ç³»ç»Ÿã€‚
åŸºäº APScheduler è°ƒåº¦é‡‡é›†ã€åˆ†æã€é€šçŸ¥ä»»åŠ¡ã€‚

ä¸‰é˜¶æ®µè§£è€¦æ¶æ„:
- é˜¶æ®µä¸€ (æŠ“å–ä¸è½åº“): è®¢é˜…æµ + æ¢ç´¢æµé‡‡é›†ï¼Œå»é‡/è´¨é‡è¯„åˆ†åè½åº“ï¼Œä¸è§¦å‘ AI åˆ†æ
- é˜¶æ®µäºŒ (ç‹¬ç«‹ AI åˆ†æ): å®šæ—¶ä»»åŠ¡æŒ‰ä¼˜å…ˆçº§æ’åºå¤„ç†æœªåˆ†æå†…å®¹ï¼Œå†™å› ai_analysis + importance
- é˜¶æ®µä¸‰ (æ±‡æ€»ä¸æ¨é€): å®šæ—¶æŒ‰æ—¶é—´çª—å£æŸ¥è¯¢å·²åˆ†æå†…å®¹ï¼ŒäºŒæ¬¡æ±‡æ€»åä¸€æ¬¡æ€§æ¨é€æ‰¹é‡ç®€æŠ¥
"""

import asyncio
import signal
import sys
from datetime import datetime, timedelta
from typing import Optional
from zoneinfo import ZoneInfo

from apscheduler.schedulers.asyncio import AsyncIOScheduler
from apscheduler.triggers.cron import CronTrigger
from apscheduler.triggers.interval import IntervalTrigger
from loguru import logger

from src.config import settings
from src.storage.database import DatabaseManager, get_db_manager
from src.subscription.manager import SubscriptionManager
from src.sources.twitter_search import TwitterSearchClient
from src.sources.twitter_detail import TwitterDetailClient
from src.sources.youtube import YouTubeClient
from src.sources.youtube_transcript import YouTubeTranscriptClient
from src.sources.rss import RSSClient
from src.analyzer.content_analyzer import ContentAnalyzer, get_content_analyzer
from src.filter.smart_filter import SmartFilter
from src.notification.client import FeishuClient
from src.notification.builder import MessageBuilder, get_local_time

# é…ç½®æ—¥å¿—
logger.remove()
logger.add(
    sys.stdout,
    format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>",
    level=settings.log_level,
)
logger.add(
    "logs/infohunter_{time:YYYY-MM-DD}.log",
    rotation="00:00",
    retention="30 days",
    level="DEBUG",
)


class InfoHunter:
    """InfoHunter ä¸»è°ƒåº¦å™¨"""

    SERVER_TZ = ZoneInfo(settings.timezone)

    def __init__(self):
        self.db: Optional[DatabaseManager] = None
        self.sub_manager: Optional[SubscriptionManager] = None
        self.twitter_search: Optional[TwitterSearchClient] = None
        self.twitter_detail: Optional[TwitterDetailClient] = None
        self.youtube: Optional[YouTubeClient] = None
        self.youtube_transcript: Optional[YouTubeTranscriptClient] = None
        self.rss: Optional[RSSClient] = None
        self.smart_filter: Optional[SmartFilter] = None
        self.analyzer: Optional[ContentAnalyzer] = None
        self.feishu: Optional[FeishuClient] = None
        self.scheduler: Optional[AsyncIOScheduler] = None
        self.running = False
        self.is_first_run = True
        # Twitter API credit è¿½è¸ª (æ¯æ—¥é‡ç½®)
        self._twitter_credits_used: int = 0
        self._twitter_credits_date: str = ""  # YYYY-MM-DD

    # ===== åŠ¨æ€é…ç½®ï¼ˆä¼˜å…ˆè¯»æ•°æ®åº“ SystemConfigï¼Œfallback åˆ° .env settingsï¼‰ =====

    def _get_db_config(self, key: str) -> Optional[dict]:
        """ä»æ•°æ®åº“ SystemConfig è¯»å–é…ç½®ï¼Œè¿”å› config_value (dict) æˆ– None"""
        if not self.db:
            return None
        try:
            cfg = self.db.get_system_config(key)
            return cfg.config_value if cfg else None
        except Exception:
            return None

    @property
    def dynamic_subscription_enabled(self) -> bool:
        cfg = self._get_db_config("subscription_config")
        if cfg and "enabled" in cfg:
            return bool(cfg["enabled"])
        return settings.subscription_enabled

    @property
    def dynamic_notify_enabled(self) -> bool:
        cfg = self._get_db_config("notify_config")
        if cfg and "enabled" in cfg:
            return bool(cfg["enabled"])
        return settings.notify_enabled

    @property
    def dynamic_explore_enabled(self) -> bool:
        cfg = self._get_db_config("explore_config")
        if cfg and "enabled" in cfg:
            return bool(cfg["enabled"])
        return settings.explore_enabled

    @property
    def dynamic_explore_twitter_woeids(self) -> str:
        cfg = self._get_db_config("explore_config")
        if cfg and cfg.get("twitter_woeids"):
            return cfg["twitter_woeids"]
        return settings.explore_twitter_woeids

    @property
    def dynamic_explore_youtube_regions(self) -> str:
        cfg = self._get_db_config("explore_config")
        if cfg and cfg.get("youtube_regions"):
            return cfg["youtube_regions"]
        return settings.explore_youtube_regions

    @property
    def dynamic_explore_keywords(self) -> str:
        cfg = self._get_db_config("explore_keywords")
        if cfg and cfg.get("keywords"):
            return cfg["keywords"]
        return settings.explore_keywords

    @property
    def dynamic_notify_schedule(self) -> str:
        cfg = self._get_db_config("notify_schedule")
        if cfg and cfg.get("schedule"):
            return cfg["schedule"]
        return settings.notify_schedule

    @property
    def dynamic_analysis_focus(self) -> str:
        cfg = self._get_db_config("analysis_focus")
        if cfg and cfg.get("focus"):
            return cfg["focus"]
        return "comprehensive"

    @property
    def dynamic_min_quality_score(self) -> float:
        cfg = self._get_db_config("min_quality_score")
        if cfg and "value" in cfg:
            try:
                return float(cfg["value"])
            except (ValueError, TypeError):
                pass
        return settings.min_quality_score

    @property
    def dynamic_explore_interval(self) -> int:
        """å…³é”®è¯æœç´¢é—´éš” (å…¼å®¹æ—§å­—æ®µå)"""
        return self.dynamic_explore_keyword_interval

    @property
    def dynamic_explore_keyword_interval(self) -> int:
        cfg = self._get_db_config("explore_config")
        if cfg and cfg.get("keyword_interval"):
            try:
                return int(cfg["keyword_interval"])
            except (ValueError, TypeError):
                pass
        if cfg and cfg.get("interval"):
            try:
                return int(cfg["interval"])
            except (ValueError, TypeError):
                pass
        return settings.explore_fetch_interval

    @property
    def dynamic_explore_trend_interval(self) -> int:
        cfg = self._get_db_config("explore_config")
        if cfg and cfg.get("trend_interval"):
            try:
                return int(cfg["trend_interval"])
            except (ValueError, TypeError):
                pass
        return settings.explore_trend_interval

    @property
    def dynamic_max_trends_per_woeid(self) -> int:
        cfg = self._get_db_config("explore_config")
        if cfg and cfg.get("max_trends_per_woeid"):
            try:
                return int(cfg["max_trends_per_woeid"])
            except (ValueError, TypeError):
                pass
        return settings.explore_max_trends_per_woeid

    @property
    def dynamic_max_search_per_keyword(self) -> int:
        cfg = self._get_db_config("explore_config")
        if cfg and cfg.get("max_search_per_keyword"):
            try:
                return int(cfg["max_search_per_keyword"])
            except (ValueError, TypeError):
                pass
        return settings.explore_max_search_per_keyword

    @property
    def dynamic_twitter_daily_credit_limit(self) -> int:
        # ä¼˜å…ˆï¼šç‹¬ç«‹ key
        limit_cfg = self._get_db_config("twitter_credit_limit")
        if limit_cfg and limit_cfg.get("daily_limit") is not None:
            try:
                return int(limit_cfg["daily_limit"])
            except (ValueError, TypeError):
                pass
        # å‘åå…¼å®¹ï¼šexplore_config ä¸­çš„æ—§å­—æ®µ
        cfg = self._get_db_config("explore_config")
        if cfg and cfg.get("twitter_daily_credit_limit"):
            try:
                return int(cfg["twitter_daily_credit_limit"])
            except (ValueError, TypeError):
                pass
        return settings.twitter_daily_credit_limit

    def _track_twitter_credits(
        self,
        credits: int,
        operation: str = "unknown",
        detail: str = "",
        context: str = "explore",
    ) -> None:
        """è¿½è¸ª Twitter API credit æ¶ˆè€—å¹¶æŒä¹…åŒ–åˆ°æ•°æ®åº“"""
        today = datetime.now(self.SERVER_TZ).strftime("%Y-%m-%d")
        if self._twitter_credits_date != today:
            self._twitter_credits_used = 0
            self._twitter_credits_date = today
        self._twitter_credits_used += credits
        logger.debug(f"Twitter credit: +{credits}, ä»Šæ—¥ç´¯è®¡: {self._twitter_credits_used}")

        if self.db:
            try:
                self.db.log_credit_usage(
                    source="twitter",
                    operation=operation,
                    credits=credits,
                    detail=detail or None,
                    context=context,
                )
            except Exception as e:
                logger.warning(f"æŒä¹…åŒ– credit è®°å½•å¤±è´¥: {e}")

    def _check_twitter_credit_budget(self, estimated_cost: int = 0) -> bool:
        """æ£€æŸ¥æ˜¯å¦è¶…å‡ºæ¯æ—¥ credit é¢„ç®—ï¼ˆç»“åˆå†…å­˜å’Œæ•°æ®åº“è®°å½•ï¼‰"""
        limit = self.dynamic_twitter_daily_credit_limit
        if limit <= 0:
            return True
        today = datetime.now(self.SERVER_TZ).strftime("%Y-%m-%d")
        if self._twitter_credits_date != today:
            # æ–°çš„ä¸€å¤©ï¼šä»æ•°æ®åº“æ¢å¤å·²ç”¨ creditï¼ˆé˜²æ­¢é‡å¯ä¸¢å¤±ï¼‰
            if self.db:
                try:
                    self._twitter_credits_used = self.db.get_credit_usage_today(source="twitter")
                except Exception:
                    self._twitter_credits_used = 0
            else:
                self._twitter_credits_used = 0
            self._twitter_credits_date = today
        if self._twitter_credits_used + estimated_cost > limit:
            logger.warning(
                f"Twitter credit é¢„ç®—ä¸è¶³: å·²ç”¨ {self._twitter_credits_used}, "
                f"é¢„ä¼° +{estimated_cost}, ä¸Šé™ {limit}"
            )
            return False
        return True

    async def init(self) -> None:
        """åˆå§‹åŒ–å„ç»„ä»¶"""
        logger.info("åˆå§‹åŒ– InfoHunter...")

        # æ•°æ®åº“
        self.db = get_db_manager()
        self.db.init_db()
        logger.info("æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ")

        # è®¢é˜…ç®¡ç†
        self.sub_manager = SubscriptionManager(self.db)

        # è¿ç§»ï¼šç»Ÿä¸€è¿‡çŸ­çš„è®¢é˜…é—´éš”ä¸º 6h
        self._normalize_subscription_intervals()

        # æ•°æ®æº
        if settings.twitterapi_io_key:
            self.twitter_search = TwitterSearchClient()
            logger.info("TwitterAPI.io å®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆ")
        else:
            logger.warning("TwitterAPI.io æœªé…ç½®")

        if settings.scrapecreators_api_key:
            self.twitter_detail = TwitterDetailClient()
            self.youtube_transcript = YouTubeTranscriptClient()
            logger.info("ScrapeCreators å®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆ (Twitterè¯¦æƒ… + YouTubeå­—å¹•)")
        else:
            logger.warning("ScrapeCreators æœªé…ç½®")

        if settings.youtube_api_key or settings.youtube_oauth_refresh_token:
            self.youtube = YouTubeClient()
            logger.info("YouTube Data API v3 å®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆ")
        else:
            logger.warning("YouTube Data API æœªé…ç½®")

        self.rss = RSSClient()
        logger.info("RSSHub å®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆ")

        # æ™ºèƒ½è¿‡æ»¤å™¨
        self.smart_filter = SmartFilter(self.db)
        logger.info("æ™ºèƒ½è¿‡æ»¤å™¨åˆå§‹åŒ–å®Œæˆ")

        # AI åˆ†æ
        if settings.knot_enabled:
            self.analyzer = get_content_analyzer()
            logger.info("AI åˆ†æå¼•æ“åˆå§‹åŒ–å®Œæˆ")
        else:
            logger.info("AI åˆ†ææœªå¯ç”¨")

        # é£ä¹¦é€šçŸ¥
        if settings.feishu_enabled and settings.feishu_webhook_url:
            try:
                self.feishu = FeishuClient()
                logger.info("é£ä¹¦å®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆ")
            except Exception as e:
                logger.warning(f"é£ä¹¦åˆå§‹åŒ–å¤±è´¥: {e}")
        else:
            logger.warning("é£ä¹¦é€šçŸ¥æœªé…ç½®")

        # è°ƒåº¦å™¨
        self.scheduler = AsyncIOScheduler()

    def _normalize_subscription_intervals(self) -> None:
        """å°†è¿‡çŸ­çš„è®¢é˜…é—´éš”ç»Ÿä¸€ä¸º 6h (21600s)"""
        min_interval = settings.default_fetch_interval  # 21600
        subs = self.sub_manager.list_all()
        updated = 0
        for sub in subs:
            if sub.fetch_interval < min_interval:
                self.db.update_subscription(sub.id, {"fetch_interval": min_interval})
                updated += 1
                logger.info(
                    f"è®¢é˜… '{sub.name}' é—´éš”ä» {sub.fetch_interval}s è°ƒæ•´ä¸º {min_interval}s"
                )
        if updated:
            logger.info(f"å·²æ ‡å‡†åŒ– {updated} ä¸ªè®¢é˜…çš„é‡‡é›†é—´éš”ä¸º {min_interval}s ({min_interval // 3600}h)")

    def _refresh_feishu_client(self) -> None:
        """æ ¹æ®æ•°æ®åº“ SystemConfig åŠ¨æ€åˆ·æ–°é£ä¹¦å®¢æˆ·ç«¯"""
        cfg = self._get_db_config("feishu_webhook")
        if not cfg or not cfg.get("url"):
            return
        db_url = cfg["url"]
        db_secret = cfg.get("secret", "")
        # å¦‚æœæ•°æ®åº“é…ç½®ä¸å½“å‰å®¢æˆ·ç«¯ä¸åŒï¼Œé‡æ–°åˆ›å»º
        if self.feishu and self.feishu.webhook_url == db_url:
            return
        try:
            self.feishu = FeishuClient(webhook_url=db_url, secret=db_secret)
            logger.info(f"é£ä¹¦å®¢æˆ·ç«¯å·²ä»æ•°æ®åº“é…ç½®åˆ·æ–°: {db_url[:50]}...")
        except Exception as e:
            logger.warning(f"åˆ·æ–°é£ä¹¦å®¢æˆ·ç«¯å¤±è´¥: {e}")

    # ========== è®¢é˜…æµ (Following) ==========

    async def fetch_subscription(self, sub) -> None:
        """æ‰§è¡Œå•ä¸ªè®¢é˜…çš„é‡‡é›†ä»»åŠ¡ (ä¸å†ç›´æ¥æ¨é€)"""
        started_at = datetime.now()
        logger.info(f"å¼€å§‹é‡‡é›†è®¢é˜… [{sub.source}] {sub.name}: {sub.target}")

        try:
            items = []

            if sub.source == "twitter":
                items = await self._fetch_twitter(sub)
            elif sub.source == "youtube":
                items = await self._fetch_youtube(sub)
            elif sub.source == "blog":
                items = await self._fetch_blog(sub)

            if not items:
                logger.info(f"è®¢é˜… {sub.name}: æœªè·å–åˆ°æ–°å†…å®¹")
                self.db.log_fetch(
                    subscription_id=sub.id,
                    source=sub.source,
                    status="success",
                    total_fetched=0,
                    started_at=started_at,
                )
                self.sub_manager.mark_fetched(sub.id)
                return

            for item in items:
                item["subscription_id"] = sub.id

            # æ™ºèƒ½è¿‡æ»¤ (å»é‡ + è´¨é‡è¯„åˆ† + è¿‡æ»¤)
            original_count = len(items)
            if self.smart_filter:
                filtered = self.smart_filter.filter_batch(
                    items, subscription_id=sub.id
                )
            else:
                for item in items:
                    item["quality_score"] = self._calc_quality_score(item)
                min_quality = self.dynamic_min_quality_score
                filtered = [i for i in items if (i.get("quality_score", 0) >= min_quality)]

            filtered_count = original_count - len(filtered)

            # ä¿å­˜åˆ°æ•°æ®åº“
            new_count, updated_count = self.db.save_contents_batch(filtered)

            logger.info(
                f"è®¢é˜… {sub.name}: è·å– {len(items)}, "
                f"è¿‡æ»¤ {filtered_count}, æ–°å¢ {new_count}, æ›´æ–° {updated_count}"
            )

            self.db.log_fetch(
                subscription_id=sub.id,
                source=sub.source,
                status="success",
                total_fetched=len(items),
                new_items=new_count,
                filtered_items=filtered_count,
                started_at=started_at,
            )

            self.sub_manager.mark_fetched(sub.id)

        except Exception as e:
            logger.error(f"é‡‡é›†è®¢é˜… {sub.name} å¤±è´¥: {e}")
            self.db.log_fetch(
                subscription_id=sub.id,
                source=sub.source,
                status="failed",
                error_message=str(e),
                started_at=started_at,
            )

    async def _fetch_twitter(self, sub) -> list[dict]:
        """æ‰§è¡Œ Twitter é‡‡é›† (å¸¦ credit è¿½è¸ª)"""
        items = []

        if sub.type == "keyword" or sub.type == "topic":
            if self.twitter_search:
                if not self._check_twitter_credit_budget(75):
                    logger.warning(f"Twitter credit é¢„ç®—ä¸è¶³ï¼Œè·³è¿‡è®¢é˜… {sub.target}")
                    return items
                sort = "Top"
                if sub.filters and sub.filters.get("sort"):
                    sort = sub.filters["sort"]
                items = await self.twitter_search.search(
                    query=sub.target,
                    limit=20,
                    sort=sort,
                )
                self._track_twitter_credits(
                    75, operation="keyword_search",
                    detail=sub.target[:200], context="subscription",
                )
            else:
                logger.warning("TwitterAPI.io æœªé…ç½®ï¼Œæ— æ³•æ‰§è¡Œå…³é”®è¯æœç´¢")

        elif sub.type == "author":
            username = sub.target.lstrip("@")

            rss_items = await self.rss.get_author_content(
                author_id=username, platform="twitter"
            )
            if rss_items:
                items = rss_items
            elif self.twitter_search:
                if not self._check_twitter_credit_budget(75):
                    logger.warning(f"Twitter credit é¢„ç®—ä¸è¶³ï¼Œè·³è¿‡åšä¸» {username}")
                    return items
                items = await self.twitter_search.get_author_content(
                    author_id=username, limit=20
                )
                self._track_twitter_credits(
                    75, operation="author_search",
                    detail=username, context="subscription",
                )

        return items

    async def _fetch_youtube(self, sub) -> list[dict]:
        """æ‰§è¡Œ YouTube é‡‡é›†"""
        items = []

        if sub.type == "keyword" or sub.type == "topic":
            if self.youtube:
                order = "relevance"
                if sub.filters and sub.filters.get("order"):
                    order = sub.filters["order"]
                items = await self.youtube.search(
                    query=sub.target,
                    limit=20,
                    order=order,
                )
            elif self.youtube_transcript:
                items = await self.youtube_transcript.search(
                    query=sub.target,
                    limit=20,
                )
            else:
                logger.warning("YouTube æ•°æ®æºå‡æœªé…ç½®")

        elif sub.type == "author":
            channel_id = sub.target

            if self.youtube:
                items = await self.youtube.get_author_content(
                    author_id=channel_id,
                    limit=20,
                )
            elif self.youtube_transcript:
                items = await self.youtube_transcript.get_author_content(
                    author_id=channel_id,
                    limit=20,
                )
            else:
                rss_items = await self.rss.get_author_content(
                    author_id=channel_id, platform="youtube"
                )
                if rss_items:
                    items = rss_items

        # ä¸ºé«˜è´¨é‡è§†é¢‘è·å–å­—å¹•
        if items and self.youtube_transcript:
            await self._enrich_youtube_transcripts(items)

        return items

    async def _fetch_blog(self, sub) -> list[dict]:
        """æ‰§è¡Œ Blog/RSS é‡‡é›†"""
        if sub.type != "feed" or not sub.target:
            logger.warning(f"Blog è®¢é˜… {sub.name}: ç±»å‹æˆ–ç›®æ ‡æ— æ•ˆ (type={sub.type})")
            return []

        items = await self.rss.fetch_feed(
            feed_url=sub.target,
            limit=20,
            source="blog",
            author=sub.name,
        )
        return items

    async def _enrich_youtube_transcripts(self, items: list[dict]) -> None:
        """ä¸ºé«˜è´¨é‡ YouTube è§†é¢‘è·å–å­—å¹•"""
        if not self.youtube_transcript:
            return

        for item in items[:5]:
            views = item.get("metrics", {}).get("views", 0)
            likes = item.get("metrics", {}).get("likes", 0)

            if views > 1000 or likes > 50:
                video_id = item.get("content_id")
                if video_id:
                    try:
                        transcript = await self.youtube_transcript.get_transcript(video_id)
                        if transcript:
                            item["transcript"] = transcript
                            logger.debug(f"è·å–å­—å¹•æˆåŠŸ: {video_id} ({len(transcript)} chars)")
                    except Exception as e:
                        logger.debug(f"è·å–å­—å¹•å¤±è´¥: {video_id}: {e}")

    # ========== æ¢ç´¢æµ (Explore/Discover) ==========

    async def _explore_trends_job(self) -> None:
        """è°ƒåº¦å™¨å…¥å£ â€” è¶‹åŠ¿å‘ç° (ä½é¢‘, é»˜è®¤ 24h)
        åŒ…å«: Twitter è¶‹åŠ¿ + YouTube çƒ­é—¨
        """
        if not self.dynamic_explore_enabled:
            logger.debug("æ¢ç´¢æµæœªå¯ç”¨ï¼Œè·³è¿‡è¶‹åŠ¿å‘ç°")
            return

        logger.info("å¼€å§‹è¶‹åŠ¿å‘ç°...")
        total_new = 0
        total_new += await self._explore_twitter_trends()
        total_new += await self._explore_youtube_trending()
        logger.info(f"è¶‹åŠ¿å‘ç°å®Œæˆ: æ–°å¢ {total_new} æ¡å†…å®¹")

    async def _explore_keywords_job(self) -> None:
        """è°ƒåº¦å™¨å…¥å£ â€” å…³é”®è¯æ¢ç´¢ (ä¸­é¢‘, é»˜è®¤ 6h)
        åŒ…å«: ç”¨æˆ·è‡ªå®šä¹‰å…³é”®è¯åœ¨ Twitter + YouTube æœç´¢
        """
        if not self.dynamic_explore_enabled:
            logger.debug("æ¢ç´¢æµæœªå¯ç”¨ï¼Œè·³è¿‡å…³é”®è¯æ¢ç´¢")
            return

        logger.info("å¼€å§‹å…³é”®è¯æ¢ç´¢...")
        total_new = await self._explore_custom_keywords()
        logger.info(f"å…³é”®è¯æ¢ç´¢å®Œæˆ: æ–°å¢ {total_new} æ¡å†…å®¹")

    async def run_explore_cycle(self) -> None:
        """æ‰§è¡Œå®Œæ•´æ¢ç´¢æµ (æ‰‹åŠ¨è§¦å‘æ—¶ä½¿ç”¨ï¼ŒåŒ…å«è¶‹åŠ¿+å…³é”®è¯)"""
        if not self.dynamic_explore_enabled:
            logger.debug("æ¢ç´¢æµæœªå¯ç”¨")
            return

        logger.info("å¼€å§‹å®Œæ•´æ¢ç´¢æµé‡‡é›†...")
        total_new = 0
        total_new += await self._explore_twitter_trends()
        total_new += await self._explore_youtube_trending()
        total_new += await self._explore_custom_keywords()
        logger.info(f"æ¢ç´¢æµé‡‡é›†å®Œæˆ: æ–°å¢ {total_new} æ¡å†…å®¹")

    async def _explore_twitter_trends(self) -> int:
        """Twitter è¶‹åŠ¿æ¢ç´¢ (å¸¦ credit é¢„ç®—æ§åˆ¶)"""
        if not self.twitter_search:
            return 0

        woeids = [
            int(w.strip())
            for w in self.dynamic_explore_twitter_woeids.split(",")
            if w.strip()
        ]
        max_trends = self.dynamic_max_trends_per_woeid
        search_limit = self.dynamic_max_search_per_keyword

        # é¢„ä¼° credit: æ¯ä¸ª WOEID è¶‹åŠ¿ ~450 + æ¯æ¬¡æœç´¢ ~75
        estimated = len(woeids) * 450 + len(woeids) * max_trends * 75
        if not self._check_twitter_credit_budget(estimated):
            logger.warning(f"Twitter è¶‹åŠ¿æ¢ç´¢: credit é¢„ç®—ä¸è¶³ï¼Œè·³è¿‡ (é¢„ä¼° {estimated})")
            return 0

        new_total = 0
        for woeid in woeids:
            try:
                trends = await self.twitter_search.get_trends(woeid=woeid, count=10)
                self._track_twitter_credits(
                    450, operation="trends",
                    detail=f"woeid={woeid}", context="explore",
                )
                if not trends:
                    continue

                for trend in trends[:max_trends]:
                    if not self._check_twitter_credit_budget(75):
                        logger.warning("Twitter credit é¢„ç®—è€—å°½ï¼Œåœæ­¢è¶‹åŠ¿æœç´¢")
                        break

                    query = trend.get("query") or trend.get("name")
                    if not query:
                        continue

                    items = await self.twitter_search.search(
                        query=query, limit=search_limit, sort="Top"
                    )
                    self._track_twitter_credits(
                        75, operation="trend_search",
                        detail=query[:200], context="explore",
                    )
                    if not items:
                        continue

                    for item in items:
                        item["subscription_id"] = None

                    if self.smart_filter:
                        items = self.smart_filter.filter_batch(items)
                    else:
                        for item in items:
                            item["quality_score"] = self._calc_quality_score(item)
                        items = [i for i in items if i.get("quality_score", 0) >= self.dynamic_min_quality_score]

                    if items:
                        new_count, _ = self.db.save_contents_batch(items)
                        new_total += new_count

            except Exception as e:
                logger.error(f"Twitter è¶‹åŠ¿æ¢ç´¢å¤±è´¥ (woeid={woeid}): {e}")

        if new_total > 0:
            logger.info(f"Twitter è¶‹åŠ¿æ¢ç´¢: æ–°å¢ {new_total} æ¡ (credit å·²ç”¨: {self._twitter_credits_used})")
        return new_total

    async def _explore_youtube_trending(self) -> int:
        """YouTube çƒ­é—¨è§†é¢‘æ¢ç´¢"""
        if not self.youtube:
            return 0

        new_total = 0
        regions = [
            r.strip()
            for r in self.dynamic_explore_youtube_regions.split(",")
            if r.strip()
        ]
        category = settings.explore_youtube_category  # category ä¸å¸¸æ”¹ï¼Œä¿æŒ .env

        for region in regions:
            try:
                items = await self.youtube.get_trending(
                    region_code=region,
                    category_id=category,
                    limit=10,
                )
                if not items:
                    continue

                for item in items:
                    item["subscription_id"] = None

                if self.smart_filter:
                    items = self.smart_filter.filter_batch(items)
                else:
                    for item in items:
                        item["quality_score"] = self._calc_quality_score(item)
                    items = [i for i in items if i.get("quality_score", 0) >= self.dynamic_min_quality_score]

                if items:
                    new_count, _ = self.db.save_contents_batch(items)
                    new_total += new_count

                # ä¸ºçƒ­é—¨è§†é¢‘è·å–å­—å¹•
                if items and self.youtube_transcript:
                    await self._enrich_youtube_transcripts(items)

            except Exception as e:
                logger.error(f"YouTube çƒ­é—¨æ¢ç´¢å¤±è´¥ (region={region}): {e}")

        if new_total > 0:
            logger.info(f"YouTube çƒ­é—¨æ¢ç´¢: æ–°å¢ {new_total} æ¡")
        return new_total

    async def _explore_custom_keywords(self) -> int:
        """ç”¨æˆ·è‡ªå®šä¹‰æ¢ç´¢å…³é”®è¯ (å¸¦ credit é¢„ç®—æ§åˆ¶)"""
        keywords = [
            k.strip()
            for k in self.dynamic_explore_keywords.split(",")
            if k.strip()
        ]
        if not keywords:
            return 0

        search_limit = self.dynamic_max_search_per_keyword
        new_total = 0

        for keyword in keywords:
            # Twitter æœç´¢ (å¸¦ credit æ£€æŸ¥)
            if self.twitter_search and self._check_twitter_credit_budget(75):
                try:
                    items = await self.twitter_search.search(
                        query=keyword, limit=search_limit, sort="Top"
                    )
                    self._track_twitter_credits(
                        75, operation="keyword_search",
                        detail=keyword[:200], context="explore",
                    )
                    for item in items:
                        item["subscription_id"] = None
                    if self.smart_filter:
                        items = self.smart_filter.filter_batch(items)
                    if items:
                        new_count, _ = self.db.save_contents_batch(items)
                        new_total += new_count
                except Exception as e:
                    logger.error(f"æ¢ç´¢å…³é”®è¯ Twitter æœç´¢å¤±è´¥ ({keyword}): {e}")

            # YouTube æœç´¢ (viewCount æ’åºè·å–çƒ­é—¨)
            if self.youtube:
                try:
                    items = await self.youtube.search(
                        query=keyword, limit=search_limit, order="viewCount"
                    )
                    for item in items:
                        item["subscription_id"] = None
                    if self.smart_filter:
                        items = self.smart_filter.filter_batch(items)
                    if items:
                        new_count, _ = self.db.save_contents_batch(items)
                        new_total += new_count
                except Exception as e:
                    logger.error(f"æ¢ç´¢å…³é”®è¯ YouTube æœç´¢å¤±è´¥ ({keyword}): {e}")

        if new_total > 0:
            logger.info(f"è‡ªå®šä¹‰æ¢ç´¢å…³é”®è¯: æ–°å¢ {new_total} æ¡")
        return new_total

    # ========== é˜¶æ®µä¸‰ï¼šæ¨é€è°ƒåº¦ (æ—¶é—´çª—å£ + æ‰¹é‡ç®€æŠ¥) ==========

    async def run_notify_batch(self) -> None:
        """å®šæ—¶æ¨é€ä»»åŠ¡ï¼ˆæ—¶é—´çª—å£ + æ‰¹é‡ç®€æŠ¥æ¨¡å¼ï¼‰

        æ ¸å¿ƒé€»è¾‘ï¼š
        1. ç¡®å®šæ—¶é—´çª—å£ [ä¸Šæ¬¡æ¨é€æ—¶é—´ ~ å½“å‰]
        2. æŸ¥è¯¢çª—å£å†…å·²åˆ†æä½†æœªæ¨é€çš„å†…å®¹
        3. æŒ‰ importance æ’åºå– TOP N
        4. å¯é€‰ï¼šè°ƒç”¨ trend_analysis Agent åšäºŒæ¬¡æ±‡æ€»
        5. æ„å»ºä¸€ä»½ç®€æŠ¥ï¼Œä¸€æ¬¡æ€§æ¨é€åˆ°é£ä¹¦
        6. æ ‡è®°æ‰€æœ‰å†…å®¹ä¸ºå·²æ¨é€
        """
        if not self.dynamic_notify_enabled:
            logger.debug("æ¨é€é€šçŸ¥æœªå¯ç”¨")
            return

        self._refresh_feishu_client()

        if not self.feishu:
            return

        try:
            now = datetime.now()
            window_start = self.db.get_last_notify_time()
            if not window_start:
                window_start = now - timedelta(hours=12)
            window_end = now

            top_n = settings.notify_top_n

            contents = self.db.get_analyzed_contents_in_window(
                window_start=window_start,
                window_end=window_end,
                notified=False,
                limit=top_n,
            )

            if not contents:
                logger.debug(
                    f"æ— å¾…æ¨é€å†…å®¹ "
                    f"(çª—å£ {window_start.strftime('%m/%d %H:%M')} ~ {window_end.strftime('%m/%d %H:%M')})"
                )
                return

            logger.info(
                f"æ¨é€ç®€æŠ¥: {len(contents)} æ¡å†…å®¹ "
                f"(çª—å£ {window_start.strftime('%m/%d %H:%M')} ~ {window_end.strftime('%m/%d %H:%M')})"
            )

            ai_trend_summary = None
            if settings.notify_enable_trend_summary and self.analyzer and len(contents) >= 3:
                try:
                    items_for_trend = [
                        {
                            "content": c.content or "",
                            "title": c.title,
                            "source": c.source,
                            "author": c.author or "",
                            "metrics": c.metrics,
                            "ai_analysis": c.ai_analysis,
                        }
                        for c in contents
                        if c.ai_analysis
                    ]
                    if items_for_trend:
                        result = await self.analyzer.analyze_batch(
                            items_for_trend, focus="briefing_summary"
                        )
                        if result["status"] == "success":
                            ai_trend_summary = result["analysis"]
                            logger.info("äºŒæ¬¡æ±‡æ€»å®Œæˆ")
                except Exception as e:
                    logger.warning(f"äºŒæ¬¡æ±‡æ€»å¤±è´¥ (ä¸å½±å“æ¨é€): {e}")

            msg = MessageBuilder.build_briefing(
                contents=contents,
                window_start=window_start,
                window_end=window_end,
                ai_trend_summary=ai_trend_summary,
            )

            success = await self.feishu.send_markdown_card(
                "ğŸ“‹ InfoHunter ç®€æŠ¥", msg
            )

            if success:
                content_ids = [c.id for c in contents]
                self.db.mark_contents_notified(content_ids)
                logger.info(f"ç®€æŠ¥æ¨é€æˆåŠŸ: {len(contents)} æ¡å†…å®¹å·²æ ‡è®°ä¸ºå·²æ¨é€")
            else:
                logger.error("ç®€æŠ¥æ¨é€å¤±è´¥")

        except Exception as e:
            logger.error(f"æ¨é€ä»»åŠ¡å¤±è´¥: {e}")

    # ========== é˜¶æ®µäºŒï¼šç‹¬ç«‹ AI åˆ†æå®šæ—¶ä»»åŠ¡ ==========

    async def run_ai_analysis_job(self) -> None:
        """ç‹¬ç«‹ AI åˆ†æå®šæ—¶ä»»åŠ¡ï¼ˆä¸æŠ“å–/æ¨é€å®Œå…¨è§£è€¦ï¼‰

        æŒ‰ä¼˜å…ˆçº§æ’åºå¤„ç†æœªåˆ†æå†…å®¹ï¼š
        1. è®¢é˜…æµï¼ˆæœ‰ subscription_idï¼‰ä¼˜å…ˆäºæ¢ç´¢æµ
        2. è¶Šæ–°çš„å†…å®¹è¶Šä¼˜å…ˆ
        3. æ¯è½®ä¸Šé™ analysis_batch_size æ¡
        """
        if not self.analyzer:
            return

        batch_size = settings.analysis_batch_size
        analysis_focus = self.dynamic_analysis_focus

        try:
            unanalyzed = self.db.get_unanalyzed_contents_prioritized(limit=batch_size)
            if not unanalyzed:
                logger.debug("æ— å¾…åˆ†æå†…å®¹")
                return

            logger.info(f"AI åˆ†æä»»åŠ¡: å¾…å¤„ç† {len(unanalyzed)} æ¡ (ä¾§é‡: {analysis_focus})")

            analyzed_count = 0
            for content in unanalyzed:
                try:
                    result = await self.analyzer.analyze_content(
                        content=content.content or "",
                        source=content.source,
                        title=content.title,
                        author=content.author,
                        metrics=content.metrics,
                        transcript=content.transcript,
                        analysis_focus=analysis_focus,
                    )

                    if result["status"] == "success" and result["analysis"]:
                        analysis = result["analysis"]
                        importance = None
                        if isinstance(analysis, dict):
                            importance = analysis.get("importance")

                        self.db.update_ai_analysis(
                            content.id, analysis, importance=importance
                        )
                        analyzed_count += 1

                except Exception as e:
                    logger.error(f"åˆ†æå†…å®¹ {content.content_id} å¤±è´¥: {e}")

            logger.info(f"AI åˆ†æå®Œæˆ: {analyzed_count}/{len(unanalyzed)} æˆåŠŸ")

        except Exception as e:
            logger.error(f"AI åˆ†æä»»åŠ¡å¤±è´¥: {e}")

    # ========== æŠ¥å‘Š ==========

    async def send_daily_report(self) -> None:
        """å‘é€æ—¥æŠ¥ (AI Newsletter æ‘˜è¦)"""
        if not self.feishu:
            return

        try:
            now = datetime.now(self.SERVER_TZ)
            since = now - timedelta(hours=24)
            since_naive = since.replace(tzinfo=None)

            contents = self.db.get_contents_for_report(since=since_naive)
            if not contents:
                logger.info("è¿‡å» 24 å°æ—¶æ— å†…å®¹ï¼Œè·³è¿‡æ—¥æŠ¥")
                return

            # AI è¶‹åŠ¿åˆ†æ (è¶‹åŠ¿é›·è¾¾ + Newsletter æ‘˜è¦)
            ai_summary = None
            if self.analyzer:
                items_for_analysis = [
                    {
                        "content": c.content or "",
                        "title": c.title,
                        "source": c.source,
                        "author": c.author or "",
                        "metrics": c.metrics,
                    }
                    for c in contents[:30]
                    if c.content
                ]
                if items_for_analysis:
                    result = await self.analyzer.analyze_batch(
                        items_for_analysis, focus="daily_newsletter"
                    )
                    if result["status"] == "success":
                        ai_summary = result["analysis"]

            contents_data = [
                {
                    "source": c.source,
                    "title": c.title,
                    "content": c.content or "",
                    "author": c.author or "",
                    "url": c.url or "",
                }
                for c in contents
            ]

            msg = MessageBuilder.build_daily_report(
                contents_data, date=now, ai_summary=ai_summary
            )

            success = await self.feishu.send_markdown_card("ğŸ“Š InfoHunter æ—¥æŠ¥", msg)
            if success:
                logger.info(f"æ—¥æŠ¥æ¨é€æˆåŠŸï¼Œå…± {len(contents)} æ¡å†…å®¹")
            else:
                logger.error("æ—¥æŠ¥æ¨é€å¤±è´¥")

        except Exception as e:
            logger.error(f"å‘é€æ—¥æŠ¥å¤±è´¥: {e}")

    async def send_weekly_report(self) -> None:
        """å‘é€å‘¨æŠ¥"""
        if not self.feishu:
            return

        try:
            now = datetime.now(self.SERVER_TZ)
            since = now - timedelta(days=7)
            since_naive = since.replace(tzinfo=None)

            contents = self.db.get_contents_for_report(since=since_naive, limit=500)
            if not contents:
                logger.info("è¿‡å» 7 å¤©æ— å†…å®¹ï¼Œè·³è¿‡å‘¨æŠ¥")
                return

            ai_summary = None
            if self.analyzer:
                items_for_analysis = [
                    {
                        "content": c.content or "",
                        "title": c.title,
                        "source": c.source,
                        "author": c.author or "",
                        "metrics": c.metrics,
                    }
                    for c in contents[:50]
                    if c.content
                ]
                if items_for_analysis:
                    result = await self.analyzer.analyze_batch(
                        items_for_analysis, focus="weekly_summary"
                    )
                    if result["status"] == "success":
                        ai_summary = result["analysis"]

            contents_data = [
                {
                    "source": c.source,
                    "title": c.title,
                    "content": c.content or "",
                    "author": c.author or "",
                    "url": c.url or "",
                }
                for c in contents
            ]

            msg = MessageBuilder.build_weekly_report(
                contents_data,
                week_start=since_naive,
                week_end=now.replace(tzinfo=None),
                ai_summary=ai_summary,
            )

            success = await self.feishu.send_markdown_card("ğŸ“Š InfoHunter å‘¨æŠ¥", msg)
            if success:
                logger.info(f"å‘¨æŠ¥æ¨é€æˆåŠŸï¼Œå…± {len(contents)} æ¡å†…å®¹")

        except Exception as e:
            logger.error(f"å‘é€å‘¨æŠ¥å¤±è´¥: {e}")

    # ========== è´¨é‡è¯„åˆ† ==========

    def _calc_quality_score(self, item: dict) -> float:
        """è®¡ç®—å†…å®¹è´¨é‡è¯„åˆ† (0-1)"""
        score = 0.0
        metrics = item.get("metrics", {})

        likes = metrics.get("likes", 0)
        retweets = metrics.get("retweets", 0)
        views = metrics.get("views", 0)
        replies = metrics.get("replies", 0)

        engagement = likes + retweets * 2 + replies * 3
        if engagement > 1000:
            score += 0.5
        elif engagement > 100:
            score += 0.3
        elif engagement > 10:
            score += 0.15
        elif engagement > 0:
            score += 0.05

        content = item.get("content", "")
        if len(content) > 200:
            score += 0.2
        elif len(content) > 50:
            score += 0.1
        elif len(content) > 10:
            score += 0.05

        if item.get("title"):
            score += 0.1

        if item.get("media_attachments"):
            score += 0.1

        if views > 100000:
            score += 0.1
        elif views > 10000:
            score += 0.05

        return min(score, 1.0)

    # ========== è°ƒåº¦å¾ªç¯ ==========

    async def run_fetch_cycle(self) -> None:
        """æ‰§è¡Œä¸€è½®è®¢é˜…æµé‡‡é›†"""
        if not self.dynamic_subscription_enabled:
            logger.debug("è®¢é˜…æµæœªå¯ç”¨")
            return

        due_subs = self.sub_manager.get_due_subscriptions()
        if not due_subs:
            logger.debug("æ— éœ€é‡‡é›†çš„è®¢é˜…")
            return

        logger.info(f"æœ¬è½®éœ€é‡‡é›† {len(due_subs)} ä¸ªè®¢é˜…")
        for sub in due_subs:
            await self.fetch_subscription(sub)

        if self.smart_filter:
            self.smart_filter.reset_seen_hashes()

    async def start(self) -> None:
        """å¯åŠ¨ InfoHunter"""
        await self.init()
        self.running = True

        now = get_local_time()
        logger.info(f"InfoHunter å¯åŠ¨ ({now.strftime('%Y-%m-%d %H:%M')} {settings.timezone})")

        # 1. è®¢é˜…æµé‡‡é›†è°ƒåº¦ (é»˜è®¤æ¯ 30 åˆ†é’Ÿæ£€æŸ¥åˆ°æœŸè®¢é˜…)
        fetch_check_minutes = max(settings.fetch_check_interval // 60, 5)
        self.scheduler.add_job(
            self.run_fetch_cycle,
            trigger=IntervalTrigger(minutes=fetch_check_minutes),
            id="fetch_cycle",
            name="è®¢é˜…æµé‡‡é›†",
            replace_existing=True,
        )
        logger.info(
            f"è®¢é˜…æµ: {'å·²å¯ç”¨' if self.dynamic_subscription_enabled else 'å·²å…³é—­'} "
            f"(æ£€æŸ¥é—´éš” {fetch_check_minutes}min)"
        )

        # 2. æ¢ç´¢æµ â€” è¶‹åŠ¿å‘ç° (ä½é¢‘ï¼Œé»˜è®¤ 24hï¼Œæ¶ˆè€—å¤§é‡ credit)
        explore_trend_hours = max(self.dynamic_explore_trend_interval // 3600, 1)
        self.scheduler.add_job(
            self._explore_trends_job,
            trigger=IntervalTrigger(hours=explore_trend_hours),
            id="explore_trends",
            name="è¶‹åŠ¿å‘ç°",
            replace_existing=True,
        )

        # 3. æ¢ç´¢æµ â€” å…³é”®è¯æœç´¢ (ä¸­é¢‘ï¼Œé»˜è®¤ 6h)
        explore_kw_minutes = max(self.dynamic_explore_keyword_interval // 60, 30)
        self.scheduler.add_job(
            self._explore_keywords_job,
            trigger=IntervalTrigger(minutes=explore_kw_minutes),
            id="explore_keywords",
            name="å…³é”®è¯æ¢ç´¢",
            replace_existing=True,
        )
        logger.info(
            f"æ¢ç´¢æµ: {'å·²å¯ç”¨' if self.dynamic_explore_enabled else 'å·²å…³é—­'} "
            f"(è¶‹åŠ¿ {explore_trend_hours}h, å…³é”®è¯ {explore_kw_minutes}min)"
        )

        # 4. ç‹¬ç«‹ AI åˆ†æå®šæ—¶ä»»åŠ¡
        if self.analyzer:
            analysis_check_minutes = max(settings.analysis_check_interval // 60, 5)
            self.scheduler.add_job(
                self.run_ai_analysis_job,
                trigger=IntervalTrigger(minutes=analysis_check_minutes),
                id="ai_analysis",
                name="AI åˆ†æ",
                replace_existing=True,
            )
            logger.info(
                f"AI åˆ†æ: å·²å¯ç”¨ (é—´éš” {analysis_check_minutes}min, "
                f"æ¯è½®ä¸Šé™ {settings.analysis_batch_size} æ¡)"
            )
        else:
            logger.info("AI åˆ†æ: æœªå¯ç”¨ (knot_enabled=false)")

        # 5. æ¨é€è°ƒåº¦ (æ—¶é—´çª—å£ + æ‰¹é‡ç®€æŠ¥ï¼Œå¯åœç”± handler åŠ¨æ€åˆ¤æ–­)
        notify_times = [
            t.strip()
            for t in self.dynamic_notify_schedule.split(",")
            if t.strip()
        ]
        for i, time_str in enumerate(notify_times):
            try:
                hour, minute = time_str.split(":")
                self.scheduler.add_job(
                    self.run_notify_batch,
                    trigger=CronTrigger(
                        hour=int(hour), minute=int(minute), timezone=self.SERVER_TZ
                    ),
                    id=f"notify_batch_{i}",
                    name=f"å®šæ—¶æ¨é€ ({time_str})",
                    replace_existing=True,
                )
            except ValueError:
                logger.warning(f"æ— æ•ˆçš„æ¨é€æ—¶é—´æ ¼å¼: {time_str}")
        logger.info(
            f"æ¨é€: {'å·²å¯ç”¨' if self.dynamic_notify_enabled else 'å·²å…³é—­'} "
            f"({', '.join(notify_times)})"
        )

        # 6. æ—¥æŠ¥ (æ¯å¤© 09:30ï¼Œåœ¨ç®€æŠ¥ä¹‹åï¼Œæä¾› 24h å…¨é‡è§†è§’)
        self.scheduler.add_job(
            self.send_daily_report,
            trigger=CronTrigger(hour=9, minute=30, timezone=self.SERVER_TZ),
            id="daily_report",
            name="æ—¥æŠ¥æ¨é€",
            replace_existing=True,
        )

        # 7. å‘¨æŠ¥ (æ¯å‘¨ä¸€ 10:00ï¼Œä¸æ—¥æŠ¥/ç®€æŠ¥é”™å¼€)
        self.scheduler.add_job(
            self.send_weekly_report,
            trigger=CronTrigger(
                day_of_week=0, hour=10, minute=0, timezone=self.SERVER_TZ
            ),
            id="weekly_report",
            name="å‘¨æŠ¥æ¨é€",
            replace_existing=True,
        )

        self.scheduler.start()

        # é¦–æ¬¡é‡‡é›† (ä»…è®¢é˜…æµï¼Œæ¢ç´¢æµç­‰å¾…è°ƒåº¦å™¨è§¦å‘)
        if self.dynamic_subscription_enabled:
            logger.info("æ‰§è¡Œé¦–æ¬¡è®¢é˜…æµé‡‡é›†...")
            await self.run_fetch_cycle()
        self.is_first_run = False
        logger.info("æ¢ç´¢æµå°†åœ¨ä¸‹ä¸€ä¸ªè°ƒåº¦å‘¨æœŸè‡ªåŠ¨æ‰§è¡Œ (ä¸åœ¨å¯åŠ¨æ—¶ç«‹å³æ‰§è¡Œä»¥èŠ‚çœ credit)")

        # ä¿æŒè¿è¡Œ
        try:
            while self.running:
                await asyncio.sleep(1)
        except asyncio.CancelledError:
            logger.info("æ”¶åˆ°åœæ­¢ä¿¡å·")

    async def stop(self) -> None:
        """ä¼˜é›…åœæ­¢ InfoHunter"""
        if not self.running:
            return
        logger.info("æ­£åœ¨åœæ­¢ InfoHunter...")
        self.running = False
        if self.scheduler and self.scheduler.running:
            self.scheduler.shutdown(wait=True)
            logger.info("APScheduler å·²åœæ­¢")
        logger.info("InfoHunter å·²åœæ­¢")


async def main():
    """ä¸»å‡½æ•°"""
    hunter = InfoHunter()

    loop = asyncio.get_event_loop()

    def signal_handler():
        logger.info("æ”¶åˆ°ç»ˆæ­¢ä¿¡å·...")
        asyncio.create_task(hunter.stop())

    for sig in (signal.SIGINT, signal.SIGTERM):
        loop.add_signal_handler(sig, signal_handler)

    try:
        await hunter.start()
    except Exception as e:
        logger.error(f"InfoHunter å¼‚å¸¸: {e}")
    finally:
        await hunter.stop()


if __name__ == "__main__":
    asyncio.run(main())
