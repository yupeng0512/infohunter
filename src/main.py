"""InfoHunter ä¸»è°ƒåº¦å™¨

å¤šæºç¤¾äº¤åª’ä½“ AI æ™ºèƒ½è®¢é˜…ç›‘æ§ç³»ç»Ÿã€‚
åŸºäº APScheduler è°ƒåº¦é‡‡é›†ã€åˆ†æã€é€šçŸ¥ä»»åŠ¡ã€‚
"""

import asyncio
import signal
import sys
from datetime import datetime, timedelta
from typing import Optional
from zoneinfo import ZoneInfo

from apscheduler.schedulers.asyncio import AsyncIOScheduler
from apscheduler.triggers.cron import CronTrigger
from apscheduler.triggers.interval import IntervalTrigger
from loguru import logger

from src.config import settings
from src.storage.database import DatabaseManager, get_db_manager
from src.subscription.manager import SubscriptionManager
from src.sources.twitter_search import TwitterSearchClient
from src.sources.twitter_detail import TwitterDetailClient
from src.sources.youtube import YouTubeClient
from src.sources.youtube_transcript import YouTubeTranscriptClient
from src.sources.rss import RSSClient
from src.analyzer.content_analyzer import ContentAnalyzer, get_content_analyzer
from src.filter.smart_filter import SmartFilter
from src.notification.client import FeishuClient
from src.notification.builder import MessageBuilder, get_local_time

# é…ç½®æ—¥å¿—
logger.remove()
logger.add(
    sys.stdout,
    format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>",
    level=settings.log_level,
)
logger.add(
    "logs/infohunter_{time:YYYY-MM-DD}.log",
    rotation="00:00",
    retention="30 days",
    level="DEBUG",
)


class InfoHunter:
    """InfoHunter ä¸»è°ƒåº¦å™¨"""

    SERVER_TZ = ZoneInfo(settings.timezone)

    def __init__(self):
        self.db: Optional[DatabaseManager] = None
        self.sub_manager: Optional[SubscriptionManager] = None
        self.twitter_search: Optional[TwitterSearchClient] = None
        self.twitter_detail: Optional[TwitterDetailClient] = None
        self.youtube: Optional[YouTubeClient] = None
        self.youtube_transcript: Optional[YouTubeTranscriptClient] = None
        self.rss: Optional[RSSClient] = None
        self.smart_filter: Optional[SmartFilter] = None
        self.analyzer: Optional[ContentAnalyzer] = None
        self.feishu: Optional[FeishuClient] = None
        self.scheduler: Optional[AsyncIOScheduler] = None
        self.running = False
        self.is_first_run = True

    async def init(self) -> None:
        """åˆå§‹åŒ–å„ç»„ä»¶"""
        logger.info("åˆå§‹åŒ– InfoHunter...")

        # æ•°æ®åº“
        self.db = get_db_manager()
        self.db.init_db()
        logger.info("æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ")

        # è®¢é˜…ç®¡ç†
        self.sub_manager = SubscriptionManager(self.db)

        # æ•°æ®æº
        if settings.twitterapi_io_key:
            self.twitter_search = TwitterSearchClient()
            logger.info("TwitterAPI.io å®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆ")
        else:
            logger.warning("TwitterAPI.io æœªé…ç½®")

        if settings.scrapecreators_api_key:
            self.twitter_detail = TwitterDetailClient()
            self.youtube_transcript = YouTubeTranscriptClient()
            logger.info("ScrapeCreators å®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆ (Twitterè¯¦æƒ… + YouTubeå­—å¹•)")
        else:
            logger.warning("ScrapeCreators æœªé…ç½®")

        if settings.youtube_api_key:
            self.youtube = YouTubeClient()
            logger.info("YouTube Data API v3 å®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆ")
        else:
            logger.warning("YouTube Data API æœªé…ç½®")

        self.rss = RSSClient()
        logger.info("RSSHub å®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆ")

        # æ™ºèƒ½è¿‡æ»¤å™¨
        self.smart_filter = SmartFilter(self.db)
        logger.info("æ™ºèƒ½è¿‡æ»¤å™¨åˆå§‹åŒ–å®Œæˆ")

        # AI åˆ†æ
        if settings.knot_enabled:
            self.analyzer = get_content_analyzer()
            logger.info("AI åˆ†æå¼•æ“åˆå§‹åŒ–å®Œæˆ")
        else:
            logger.info("AI åˆ†ææœªå¯ç”¨")

        # é£ä¹¦é€šçŸ¥
        if settings.feishu_enabled and settings.feishu_webhook_url:
            try:
                self.feishu = FeishuClient()
                logger.info("é£ä¹¦å®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆ")
            except Exception as e:
                logger.warning(f"é£ä¹¦åˆå§‹åŒ–å¤±è´¥: {e}")
        else:
            logger.warning("é£ä¹¦é€šçŸ¥æœªé…ç½®")

        # è°ƒåº¦å™¨
        self.scheduler = AsyncIOScheduler()

    async def fetch_subscription(self, sub) -> None:
        """æ‰§è¡Œå•ä¸ªè®¢é˜…çš„é‡‡é›†ä»»åŠ¡"""
        started_at = datetime.now()
        logger.info(f"å¼€å§‹é‡‡é›†è®¢é˜… [{sub.source}] {sub.name}: {sub.target}")

        try:
            items = []

            if sub.source == "twitter":
                items = await self._fetch_twitter(sub)
            elif sub.source == "youtube":
                items = await self._fetch_youtube(sub)

            if not items:
                logger.info(f"è®¢é˜… {sub.name}: æœªè·å–åˆ°æ–°å†…å®¹")
                self.db.log_fetch(
                    subscription_id=sub.id,
                    source=sub.source,
                    status="success",
                    total_fetched=0,
                    started_at=started_at,
                )
                self.sub_manager.mark_fetched(sub.id)
                return

            # ä¸ºæ¯æ¡å†…å®¹å…³è”è®¢é˜… ID
            for item in items:
                item["subscription_id"] = sub.id

            # æ™ºèƒ½è¿‡æ»¤ (å»é‡ + è´¨é‡è¯„åˆ† + è¿‡æ»¤)
            original_count = len(items)
            if self.smart_filter:
                filtered = self.smart_filter.filter_batch(
                    items, subscription_id=sub.id
                )
            else:
                # é™çº§: ç®€å•è´¨é‡è¯„åˆ†
                for item in items:
                    item["quality_score"] = self._calc_quality_score(item)
                min_quality = settings.min_quality_score
                filtered = [i for i in items if (i.get("quality_score", 0) >= min_quality)]

            filtered_count = original_count - len(filtered)

            # ä¿å­˜åˆ°æ•°æ®åº“
            new_count, updated_count = self.db.save_contents_batch(filtered)

            logger.info(
                f"è®¢é˜… {sub.name}: è·å– {len(items)}, "
                f"è¿‡æ»¤ {filtered_count}, æ–°å¢ {new_count}, æ›´æ–° {updated_count}"
            )

            # è®°å½•æ—¥å¿—
            self.db.log_fetch(
                subscription_id=sub.id,
                source=sub.source,
                status="success",
                total_fetched=len(items),
                new_items=new_count,
                filtered_items=filtered_count,
                started_at=started_at,
            )

            # æ›´æ–°é‡‡é›†æ—¶é—´
            self.sub_manager.mark_fetched(sub.id)

            # AI åˆ†æ (å¦‚æœå¯ç”¨)
            if sub.ai_analysis_enabled and self.analyzer and new_count > 0:
                await self._run_analysis()

            # å‘é€é€šçŸ¥
            if sub.notification_enabled and self.feishu and new_count > 0:
                if self.is_first_run:
                    logger.info("é¦–æ¬¡è¿è¡Œï¼Œè·³è¿‡é€šçŸ¥")
                else:
                    await self._send_notifications()

        except Exception as e:
            logger.error(f"é‡‡é›†è®¢é˜… {sub.name} å¤±è´¥: {e}")
            self.db.log_fetch(
                subscription_id=sub.id,
                source=sub.source,
                status="failed",
                error_message=str(e),
                started_at=started_at,
            )

    async def _fetch_twitter(self, sub) -> list[dict]:
        """æ‰§è¡Œ Twitter é‡‡é›†"""
        items = []

        if sub.type == "keyword" or sub.type == "topic":
            # å…³é”®è¯/è¯é¢˜æœç´¢ -> TwitterAPI.io
            if self.twitter_search:
                sort = "Latest"
                if sub.filters and sub.filters.get("sort"):
                    sort = sub.filters["sort"]
                items = await self.twitter_search.search(
                    query=sub.target,
                    limit=20,
                    sort=sort,
                )
            else:
                logger.warning("TwitterAPI.io æœªé…ç½®ï¼Œæ— æ³•æ‰§è¡Œå…³é”®è¯æœç´¢")

        elif sub.type == "author":
            # åšä¸»è®¢é˜… -> ä¼˜å…ˆ RSSHubï¼Œå¤‡ç”¨ TwitterAPI.io
            username = sub.target.lstrip("@")

            # å°è¯• RSSHub
            rss_items = await self.rss.get_author_content(
                author_id=username, platform="twitter"
            )
            if rss_items:
                items = rss_items
            elif self.twitter_search:
                # å›é€€åˆ° TwitterAPI.io
                items = await self.twitter_search.get_author_content(
                    author_id=username, limit=20
                )

        return items

    async def _fetch_youtube(self, sub) -> list[dict]:
        """æ‰§è¡Œ YouTube é‡‡é›†"""
        items = []

        if sub.type == "keyword" or sub.type == "topic":
            # å…³é”®è¯/è¯é¢˜æœç´¢ -> ä¼˜å…ˆ YouTube Data API v3
            if self.youtube:
                order = "relevance"
                if sub.filters and sub.filters.get("order"):
                    order = sub.filters["order"]
                items = await self.youtube.search(
                    query=sub.target,
                    limit=20,
                    order=order,
                )
            elif self.youtube_transcript:
                # å¤‡ç”¨: ScrapeCreators YouTube Search
                items = await self.youtube_transcript.search(
                    query=sub.target,
                    limit=20,
                )
            else:
                logger.warning("YouTube æ•°æ®æºå‡æœªé…ç½®")

        elif sub.type == "author":
            # é¢‘é“è®¢é˜… -> ä¼˜å…ˆ YouTube Data API v3
            channel_id = sub.target

            if self.youtube:
                items = await self.youtube.get_author_content(
                    author_id=channel_id,
                    limit=20,
                )
            elif self.youtube_transcript:
                items = await self.youtube_transcript.get_author_content(
                    author_id=channel_id,
                    limit=20,
                )
            else:
                # å°è¯• RSSHub
                rss_items = await self.rss.get_author_content(
                    author_id=channel_id, platform="youtube"
                )
                if rss_items:
                    items = rss_items

        # ä¸ºé«˜è´¨é‡è§†é¢‘è·å–å­—å¹• (ScrapeCreators)
        if items and self.youtube_transcript:
            await self._enrich_youtube_transcripts(items)

        return items

    async def _enrich_youtube_transcripts(self, items: list[dict]) -> None:
        """ä¸ºé«˜è´¨é‡ YouTube è§†é¢‘è·å–å­—å¹•"""
        if not self.youtube_transcript:
            return

        # åªä¸ºäº’åŠ¨é‡è¾ƒé«˜çš„è§†é¢‘è·å–å­—å¹• (èŠ‚çœ credits)
        for item in items[:5]:
            views = item.get("metrics", {}).get("views", 0)
            likes = item.get("metrics", {}).get("likes", 0)

            if views > 1000 or likes > 50:
                video_id = item.get("content_id")
                if video_id:
                    try:
                        transcript = await self.youtube_transcript.get_transcript(video_id)
                        if transcript:
                            item["transcript"] = transcript
                            logger.debug(f"è·å–å­—å¹•æˆåŠŸ: {video_id} ({len(transcript)} chars)")
                    except Exception as e:
                        logger.debug(f"è·å–å­—å¹•å¤±è´¥: {video_id}: {e}")

    def _calc_quality_score(self, item: dict) -> float:
        """è®¡ç®—å†…å®¹è´¨é‡è¯„åˆ† (0-1)"""
        score = 0.0
        metrics = item.get("metrics", {})

        # äº’åŠ¨é‡è¯„åˆ† (0-0.5)
        likes = metrics.get("likes", 0)
        retweets = metrics.get("retweets", 0)
        views = metrics.get("views", 0)
        replies = metrics.get("replies", 0)

        engagement = likes + retweets * 2 + replies * 3
        if engagement > 1000:
            score += 0.5
        elif engagement > 100:
            score += 0.3
        elif engagement > 10:
            score += 0.15
        elif engagement > 0:
            score += 0.05

        # å†…å®¹é•¿åº¦è¯„åˆ† (0-0.2)
        content = item.get("content", "")
        if len(content) > 200:
            score += 0.2
        elif len(content) > 50:
            score += 0.1
        elif len(content) > 10:
            score += 0.05

        # æœ‰æ ‡é¢˜åŠ åˆ† (YouTube) (0-0.1)
        if item.get("title"):
            score += 0.1

        # æœ‰åª’ä½“åŠ åˆ† (0-0.1)
        if item.get("media_attachments"):
            score += 0.1

        # æ’­æ”¾é‡åŠ åˆ† (YouTube) (0-0.1)
        if views > 100000:
            score += 0.1
        elif views > 10000:
            score += 0.05

        return min(score, 1.0)

    async def _run_analysis(self) -> None:
        """è¿è¡Œ AI åˆ†æ"""
        if not self.analyzer:
            return

        try:
            unanalyzed = self.db.get_unanalyzed_contents(limit=10)
            if not unanalyzed:
                return

            logger.info(f"å¼€å§‹ AI åˆ†æ {len(unanalyzed)} æ¡å†…å®¹...")

            for content in unanalyzed:
                try:
                    result = await self.analyzer.analyze_content(
                        content=content.content or "",
                        source=content.source,
                        title=content.title,
                        author=content.author,
                        metrics=content.metrics,
                        transcript=content.transcript,
                    )

                    if result["status"] == "success" and result["analysis"]:
                        self.db.update_ai_analysis(content.id, result["analysis"])

                        # ä»åˆ†æç»“æœæ›´æ–°è¯„åˆ†
                        analysis = result["analysis"]
                        if isinstance(analysis, dict) and analysis.get("importance"):
                            relevance = analysis["importance"] / 10.0
                            self.db.update_scores(
                                content.id, relevance_score=relevance
                            )

                except Exception as e:
                    logger.error(f"åˆ†æå†…å®¹ {content.content_id} å¤±è´¥: {e}")

        except Exception as e:
            logger.error(f"AI åˆ†æä»»åŠ¡å¤±è´¥: {e}")

    async def _send_notifications(self) -> None:
        """å‘é€æœªé€šçŸ¥çš„é«˜è´¨é‡å†…å®¹"""
        if not self.feishu:
            return

        try:
            threshold = settings.realtime_notify_threshold
            unnotified = self.db.get_unnotified_contents(
                limit=settings.max_realtime_per_hour,
                min_quality=threshold,
            )
            if not unnotified:
                return

            logger.info(f"å‘é€ {len(unnotified)} æ¡å†…å®¹é€šçŸ¥...")

            for content in unnotified:
                try:
                    # è·å–è®¢é˜…åç§°
                    sub_name = None
                    if content.subscription_id:
                        sub = self.sub_manager.get(content.subscription_id)
                        if sub:
                            sub_name = sub.name

                    msg = MessageBuilder.build_content_notification(
                        source=content.source,
                        title=content.title,
                        content=content.content or "",
                        author=content.author or "unknown",
                        url=content.url or "",
                        metrics=content.metrics,
                        ai_analysis=content.ai_analysis,
                        subscription_name=sub_name,
                    )

                    source_emoji = {"twitter": "ğŸ¦", "youtube": "ğŸ“º"}.get(
                        content.source, "ğŸ“°"
                    )
                    title = f"{source_emoji} InfoHunter æ–°å†…å®¹"

                    success = await self.feishu.send_markdown_card(title, msg)
                    if success:
                        self.db.mark_contents_notified([content.id])

                except Exception as e:
                    logger.error(f"å‘é€é€šçŸ¥å¤±è´¥ (content_id={content.content_id}): {e}")

        except Exception as e:
            logger.error(f"é€šçŸ¥ä»»åŠ¡å¤±è´¥: {e}")

    async def send_daily_report(self) -> None:
        """å‘é€æ—¥æŠ¥"""
        if not self.feishu:
            return

        try:
            now = datetime.now(self.SERVER_TZ)
            since = now - timedelta(hours=24)
            since_naive = since.replace(tzinfo=None)

            contents = self.db.get_contents_for_report(since=since_naive)
            if not contents:
                logger.info("è¿‡å» 24 å°æ—¶æ— å†…å®¹ï¼Œè·³è¿‡æ—¥æŠ¥")
                return

            # AI è¶‹åŠ¿åˆ†æ
            ai_summary = None
            if self.analyzer:
                items_for_analysis = [
                    {
                        "content": c.content or "",
                        "title": c.title,
                        "source": c.source,
                    }
                    for c in contents[:30]
                    if c.content
                ]
                if items_for_analysis:
                    result = await self.analyzer.analyze_batch(
                        items_for_analysis, focus="daily_summary"
                    )
                    if result["status"] == "success":
                        ai_summary = result["analysis"]

            # æ„å»ºæ¶ˆæ¯
            contents_data = [
                {
                    "source": c.source,
                    "title": c.title,
                    "content": c.content or "",
                    "author": c.author or "",
                    "url": c.url or "",
                }
                for c in contents
            ]

            msg = MessageBuilder.build_daily_report(
                contents_data, date=now, ai_summary=ai_summary
            )

            success = await self.feishu.send_markdown_card("ğŸ“Š InfoHunter æ—¥æŠ¥", msg)
            if success:
                logger.info(f"æ—¥æŠ¥æ¨é€æˆåŠŸï¼Œå…± {len(contents)} æ¡å†…å®¹")
            else:
                logger.error("æ—¥æŠ¥æ¨é€å¤±è´¥")

        except Exception as e:
            logger.error(f"å‘é€æ—¥æŠ¥å¤±è´¥: {e}")

    async def send_weekly_report(self) -> None:
        """å‘é€å‘¨æŠ¥"""
        if not self.feishu:
            return

        try:
            now = datetime.now(self.SERVER_TZ)
            since = now - timedelta(days=7)
            since_naive = since.replace(tzinfo=None)

            contents = self.db.get_contents_for_report(since=since_naive, limit=500)
            if not contents:
                logger.info("è¿‡å» 7 å¤©æ— å†…å®¹ï¼Œè·³è¿‡å‘¨æŠ¥")
                return

            # AI è¶‹åŠ¿åˆ†æ
            ai_summary = None
            if self.analyzer:
                items_for_analysis = [
                    {
                        "content": c.content or "",
                        "title": c.title,
                        "source": c.source,
                    }
                    for c in contents[:50]
                    if c.content
                ]
                if items_for_analysis:
                    result = await self.analyzer.analyze_batch(
                        items_for_analysis, focus="weekly_summary"
                    )
                    if result["status"] == "success":
                        ai_summary = result["analysis"]

            contents_data = [
                {
                    "source": c.source,
                    "title": c.title,
                    "content": c.content or "",
                    "author": c.author or "",
                    "url": c.url or "",
                }
                for c in contents
            ]

            msg = MessageBuilder.build_weekly_report(
                contents_data,
                week_start=since_naive,
                week_end=now.replace(tzinfo=None),
                ai_summary=ai_summary,
            )

            success = await self.feishu.send_markdown_card("ğŸ“Š InfoHunter å‘¨æŠ¥", msg)
            if success:
                logger.info(f"å‘¨æŠ¥æ¨é€æˆåŠŸï¼Œå…± {len(contents)} æ¡å†…å®¹")

        except Exception as e:
            logger.error(f"å‘é€å‘¨æŠ¥å¤±è´¥: {e}")

    async def run_fetch_cycle(self) -> None:
        """æ‰§è¡Œä¸€è½®é‡‡é›†"""
        due_subs = self.sub_manager.get_due_subscriptions()
        if not due_subs:
            logger.debug("æ— éœ€é‡‡é›†çš„è®¢é˜…")
            return

        logger.info(f"æœ¬è½®éœ€é‡‡é›† {len(due_subs)} ä¸ªè®¢é˜…")
        for sub in due_subs:
            await self.fetch_subscription(sub)

        # é‡ç½®è¿‡æ»¤å™¨æŒ‡çº¹ç¼“å­˜
        if self.smart_filter:
            self.smart_filter.reset_seen_hashes()

    async def start(self) -> None:
        """å¯åŠ¨ InfoHunter"""
        await self.init()
        self.running = True

        now = get_local_time()
        logger.info(f"InfoHunter å¯åŠ¨ ({now.strftime('%Y-%m-%d %H:%M')} {settings.timezone})")

        # é‡‡é›†è°ƒåº¦ (æ¯ 5 åˆ†é’Ÿæ£€æŸ¥ä¸€æ¬¡æ˜¯å¦æœ‰è®¢é˜…éœ€è¦é‡‡é›†)
        self.scheduler.add_job(
            self.run_fetch_cycle,
            trigger=IntervalTrigger(minutes=5),
            id="fetch_cycle",
            name="é‡‡é›†è°ƒåº¦",
            replace_existing=True,
        )

        # æ—¥æŠ¥ (æ¯å¤© 9:00)
        self.scheduler.add_job(
            self.send_daily_report,
            trigger=CronTrigger(hour=9, minute=0, timezone=self.SERVER_TZ),
            id="daily_report",
            name="æ—¥æŠ¥æ¨é€",
            replace_existing=True,
        )

        # å‘¨æŠ¥ (æ¯å‘¨ä¸€ 9:30)
        self.scheduler.add_job(
            self.send_weekly_report,
            trigger=CronTrigger(
                day_of_week=0, hour=9, minute=30, timezone=self.SERVER_TZ
            ),
            id="weekly_report",
            name="å‘¨æŠ¥æ¨é€",
            replace_existing=True,
        )

        self.scheduler.start()

        # é¦–æ¬¡é‡‡é›†
        logger.info("æ‰§è¡Œé¦–æ¬¡é‡‡é›†...")
        await self.run_fetch_cycle()
        self.is_first_run = False

        # ä¿æŒè¿è¡Œ
        try:
            while self.running:
                await asyncio.sleep(1)
        except asyncio.CancelledError:
            logger.info("æ”¶åˆ°åœæ­¢ä¿¡å·")

    async def stop(self) -> None:
        """åœæ­¢ InfoHunter"""
        logger.info("æ­£åœ¨åœæ­¢ InfoHunter...")
        self.running = False
        if self.scheduler:
            self.scheduler.shutdown(wait=False)
        logger.info("InfoHunter å·²åœæ­¢")


async def main():
    """ä¸»å‡½æ•°"""
    hunter = InfoHunter()

    loop = asyncio.get_event_loop()

    def signal_handler():
        logger.info("æ”¶åˆ°ç»ˆæ­¢ä¿¡å·...")
        asyncio.create_task(hunter.stop())

    for sig in (signal.SIGINT, signal.SIGTERM):
        loop.add_signal_handler(sig, signal_handler)

    try:
        await hunter.start()
    except Exception as e:
        logger.error(f"InfoHunter å¼‚å¸¸: {e}")
    finally:
        await hunter.stop()


if __name__ == "__main__":
    asyncio.run(main())
