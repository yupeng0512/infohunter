"""InfoHunter ä¸»è°ƒåº¦å™¨

å¤šæºç¤¾äº¤åª’ä½“ AI æ™ºèƒ½è®¢é˜…ç›‘æ§ç³»ç»Ÿã€‚
åŸºäº APScheduler è°ƒåº¦é‡‡é›†ã€åˆ†æã€é€šçŸ¥ä»»åŠ¡ã€‚

æ¶æ„:
- è®¢é˜…æµ (Following): ç”¨æˆ·åˆ›å»ºçš„å…³é”®è¯/åšä¸»/è¯é¢˜è®¢é˜…ï¼Œå®šæœŸé‡‡é›†
- æ¢ç´¢æµ (Explore): ç³»ç»Ÿè‡ªåŠ¨å‘ç°çƒ­é—¨è¶‹åŠ¿ + ç”¨æˆ·è‡ªå®šä¹‰æ¢ç´¢å…³é”®è¯
- æ¨é€è°ƒåº¦: ä¸æŠ“å–è§£è€¦ï¼ŒæŒ‰å›ºå®šæ—¶é—´ç‚¹æ±‡æ€»æ¨é€
- AI åˆ†æ: å†…å®¹åˆ†æã€è¶‹åŠ¿é›·è¾¾ã€æ™ºèƒ½æ¨èã€Newsletter æ‘˜è¦
"""

import asyncio
import signal
import sys
from datetime import datetime, timedelta
from typing import Optional
from zoneinfo import ZoneInfo

from apscheduler.schedulers.asyncio import AsyncIOScheduler
from apscheduler.triggers.cron import CronTrigger
from apscheduler.triggers.interval import IntervalTrigger
from loguru import logger

from src.config import settings
from src.storage.database import DatabaseManager, get_db_manager
from src.subscription.manager import SubscriptionManager
from src.sources.twitter_search import TwitterSearchClient
from src.sources.twitter_detail import TwitterDetailClient
from src.sources.youtube import YouTubeClient
from src.sources.youtube_transcript import YouTubeTranscriptClient
from src.sources.rss import RSSClient
from src.analyzer.content_analyzer import ContentAnalyzer, get_content_analyzer
from src.filter.smart_filter import SmartFilter
from src.notification.client import FeishuClient
from src.notification.builder import MessageBuilder, get_local_time

# é…ç½®æ—¥å¿—
logger.remove()
logger.add(
    sys.stdout,
    format="<green>{time:YYYY-MM-DD HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{name}</cyan>:<cyan>{function}</cyan>:<cyan>{line}</cyan> - <level>{message}</level>",
    level=settings.log_level,
)
logger.add(
    "logs/infohunter_{time:YYYY-MM-DD}.log",
    rotation="00:00",
    retention="30 days",
    level="DEBUG",
)


class InfoHunter:
    """InfoHunter ä¸»è°ƒåº¦å™¨"""

    SERVER_TZ = ZoneInfo(settings.timezone)

    def __init__(self):
        self.db: Optional[DatabaseManager] = None
        self.sub_manager: Optional[SubscriptionManager] = None
        self.twitter_search: Optional[TwitterSearchClient] = None
        self.twitter_detail: Optional[TwitterDetailClient] = None
        self.youtube: Optional[YouTubeClient] = None
        self.youtube_transcript: Optional[YouTubeTranscriptClient] = None
        self.rss: Optional[RSSClient] = None
        self.smart_filter: Optional[SmartFilter] = None
        self.analyzer: Optional[ContentAnalyzer] = None
        self.feishu: Optional[FeishuClient] = None
        self.scheduler: Optional[AsyncIOScheduler] = None
        self.running = False
        self.is_first_run = True
        # Twitter API credit è¿½è¸ª (æ¯æ—¥é‡ç½®)
        self._twitter_credits_used: int = 0
        self._twitter_credits_date: str = ""  # YYYY-MM-DD

    # ===== åŠ¨æ€é…ç½®ï¼ˆä¼˜å…ˆè¯»æ•°æ®åº“ SystemConfigï¼Œfallback åˆ° .env settingsï¼‰ =====

    def _get_db_config(self, key: str) -> Optional[dict]:
        """ä»æ•°æ®åº“ SystemConfig è¯»å–é…ç½®ï¼Œè¿”å› config_value (dict) æˆ– None"""
        if not self.db:
            return None
        try:
            cfg = self.db.get_system_config(key)
            return cfg.config_value if cfg else None
        except Exception:
            return None

    @property
    def dynamic_explore_enabled(self) -> bool:
        cfg = self._get_db_config("explore_config")
        if cfg and "enabled" in cfg:
            return bool(cfg["enabled"])
        return settings.explore_enabled

    @property
    def dynamic_explore_twitter_woeids(self) -> str:
        cfg = self._get_db_config("explore_config")
        if cfg and cfg.get("twitter_woeids"):
            return cfg["twitter_woeids"]
        return settings.explore_twitter_woeids

    @property
    def dynamic_explore_youtube_regions(self) -> str:
        cfg = self._get_db_config("explore_config")
        if cfg and cfg.get("youtube_regions"):
            return cfg["youtube_regions"]
        return settings.explore_youtube_regions

    @property
    def dynamic_explore_keywords(self) -> str:
        cfg = self._get_db_config("explore_keywords")
        if cfg and cfg.get("keywords"):
            return cfg["keywords"]
        return settings.explore_keywords

    @property
    def dynamic_notify_schedule(self) -> str:
        cfg = self._get_db_config("notify_schedule")
        if cfg and cfg.get("schedule"):
            return cfg["schedule"]
        return settings.notify_schedule

    @property
    def dynamic_min_quality_score(self) -> float:
        cfg = self._get_db_config("min_quality_score")
        if cfg and "value" in cfg:
            try:
                return float(cfg["value"])
            except (ValueError, TypeError):
                pass
        return settings.min_quality_score

    @property
    def dynamic_explore_interval(self) -> int:
        cfg = self._get_db_config("explore_config")
        if cfg and cfg.get("interval"):
            try:
                return int(cfg["interval"])
            except (ValueError, TypeError):
                pass
        return settings.explore_fetch_interval

    @property
    def dynamic_max_trends_per_woeid(self) -> int:
        cfg = self._get_db_config("explore_config")
        if cfg and cfg.get("max_trends_per_woeid"):
            try:
                return int(cfg["max_trends_per_woeid"])
            except (ValueError, TypeError):
                pass
        return settings.explore_max_trends_per_woeid

    @property
    def dynamic_max_search_per_keyword(self) -> int:
        cfg = self._get_db_config("explore_config")
        if cfg and cfg.get("max_search_per_keyword"):
            try:
                return int(cfg["max_search_per_keyword"])
            except (ValueError, TypeError):
                pass
        return settings.explore_max_search_per_keyword

    @property
    def dynamic_twitter_daily_credit_limit(self) -> int:
        cfg = self._get_db_config("explore_config")
        if cfg and cfg.get("twitter_daily_credit_limit"):
            try:
                return int(cfg["twitter_daily_credit_limit"])
            except (ValueError, TypeError):
                pass
        return settings.twitter_daily_credit_limit

    def _track_twitter_credits(self, credits: int) -> None:
        """è¿½è¸ª Twitter API credit æ¶ˆè€—"""
        today = datetime.now(self.SERVER_TZ).strftime("%Y-%m-%d")
        if self._twitter_credits_date != today:
            self._twitter_credits_used = 0
            self._twitter_credits_date = today
        self._twitter_credits_used += credits
        logger.debug(f"Twitter credit: +{credits}, ä»Šæ—¥ç´¯è®¡: {self._twitter_credits_used}")

    def _check_twitter_credit_budget(self, estimated_cost: int = 0) -> bool:
        """æ£€æŸ¥æ˜¯å¦è¶…å‡ºæ¯æ—¥ credit é¢„ç®—"""
        limit = self.dynamic_twitter_daily_credit_limit
        if limit <= 0:
            return True  # ä¸é™åˆ¶
        today = datetime.now(self.SERVER_TZ).strftime("%Y-%m-%d")
        if self._twitter_credits_date != today:
            self._twitter_credits_used = 0
            self._twitter_credits_date = today
        if self._twitter_credits_used + estimated_cost > limit:
            logger.warning(
                f"Twitter credit é¢„ç®—ä¸è¶³: å·²ç”¨ {self._twitter_credits_used}, "
                f"é¢„ä¼° +{estimated_cost}, ä¸Šé™ {limit}"
            )
            return False
        return True

    async def init(self) -> None:
        """åˆå§‹åŒ–å„ç»„ä»¶"""
        logger.info("åˆå§‹åŒ– InfoHunter...")

        # æ•°æ®åº“
        self.db = get_db_manager()
        self.db.init_db()
        logger.info("æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ")

        # è®¢é˜…ç®¡ç†
        self.sub_manager = SubscriptionManager(self.db)

        # æ•°æ®æº
        if settings.twitterapi_io_key:
            self.twitter_search = TwitterSearchClient()
            logger.info("TwitterAPI.io å®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆ")
        else:
            logger.warning("TwitterAPI.io æœªé…ç½®")

        if settings.scrapecreators_api_key:
            self.twitter_detail = TwitterDetailClient()
            self.youtube_transcript = YouTubeTranscriptClient()
            logger.info("ScrapeCreators å®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆ (Twitterè¯¦æƒ… + YouTubeå­—å¹•)")
        else:
            logger.warning("ScrapeCreators æœªé…ç½®")

        if settings.youtube_api_key or settings.youtube_oauth_refresh_token:
            self.youtube = YouTubeClient()
            logger.info("YouTube Data API v3 å®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆ")
        else:
            logger.warning("YouTube Data API æœªé…ç½®")

        self.rss = RSSClient()
        logger.info("RSSHub å®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆ")

        # æ™ºèƒ½è¿‡æ»¤å™¨
        self.smart_filter = SmartFilter(self.db)
        logger.info("æ™ºèƒ½è¿‡æ»¤å™¨åˆå§‹åŒ–å®Œæˆ")

        # AI åˆ†æ
        if settings.knot_enabled:
            self.analyzer = get_content_analyzer()
            logger.info("AI åˆ†æå¼•æ“åˆå§‹åŒ–å®Œæˆ")
        else:
            logger.info("AI åˆ†ææœªå¯ç”¨")

        # é£ä¹¦é€šçŸ¥
        if settings.feishu_enabled and settings.feishu_webhook_url:
            try:
                self.feishu = FeishuClient()
                logger.info("é£ä¹¦å®¢æˆ·ç«¯åˆå§‹åŒ–å®Œæˆ")
            except Exception as e:
                logger.warning(f"é£ä¹¦åˆå§‹åŒ–å¤±è´¥: {e}")
        else:
            logger.warning("é£ä¹¦é€šçŸ¥æœªé…ç½®")

        # è°ƒåº¦å™¨
        self.scheduler = AsyncIOScheduler()

    def _refresh_feishu_client(self) -> None:
        """æ ¹æ®æ•°æ®åº“ SystemConfig åŠ¨æ€åˆ·æ–°é£ä¹¦å®¢æˆ·ç«¯"""
        cfg = self._get_db_config("feishu_webhook")
        if not cfg or not cfg.get("url"):
            return
        db_url = cfg["url"]
        db_secret = cfg.get("secret", "")
        # å¦‚æœæ•°æ®åº“é…ç½®ä¸å½“å‰å®¢æˆ·ç«¯ä¸åŒï¼Œé‡æ–°åˆ›å»º
        if self.feishu and self.feishu.webhook_url == db_url:
            return
        try:
            self.feishu = FeishuClient(webhook_url=db_url, secret=db_secret)
            logger.info(f"é£ä¹¦å®¢æˆ·ç«¯å·²ä»æ•°æ®åº“é…ç½®åˆ·æ–°: {db_url[:50]}...")
        except Exception as e:
            logger.warning(f"åˆ·æ–°é£ä¹¦å®¢æˆ·ç«¯å¤±è´¥: {e}")

    # ========== è®¢é˜…æµ (Following) ==========

    async def fetch_subscription(self, sub) -> None:
        """æ‰§è¡Œå•ä¸ªè®¢é˜…çš„é‡‡é›†ä»»åŠ¡ (ä¸å†ç›´æ¥æ¨é€)"""
        started_at = datetime.now()
        logger.info(f"å¼€å§‹é‡‡é›†è®¢é˜… [{sub.source}] {sub.name}: {sub.target}")

        try:
            items = []

            if sub.source == "twitter":
                items = await self._fetch_twitter(sub)
            elif sub.source == "youtube":
                items = await self._fetch_youtube(sub)

            if not items:
                logger.info(f"è®¢é˜… {sub.name}: æœªè·å–åˆ°æ–°å†…å®¹")
                self.db.log_fetch(
                    subscription_id=sub.id,
                    source=sub.source,
                    status="success",
                    total_fetched=0,
                    started_at=started_at,
                )
                self.sub_manager.mark_fetched(sub.id)
                return

            for item in items:
                item["subscription_id"] = sub.id

            # æ™ºèƒ½è¿‡æ»¤ (å»é‡ + è´¨é‡è¯„åˆ† + è¿‡æ»¤)
            original_count = len(items)
            if self.smart_filter:
                filtered = self.smart_filter.filter_batch(
                    items, subscription_id=sub.id
                )
            else:
                for item in items:
                    item["quality_score"] = self._calc_quality_score(item)
                min_quality = self.dynamic_min_quality_score
                filtered = [i for i in items if (i.get("quality_score", 0) >= min_quality)]

            filtered_count = original_count - len(filtered)

            # ä¿å­˜åˆ°æ•°æ®åº“
            new_count, updated_count = self.db.save_contents_batch(filtered)

            logger.info(
                f"è®¢é˜… {sub.name}: è·å– {len(items)}, "
                f"è¿‡æ»¤ {filtered_count}, æ–°å¢ {new_count}, æ›´æ–° {updated_count}"
            )

            self.db.log_fetch(
                subscription_id=sub.id,
                source=sub.source,
                status="success",
                total_fetched=len(items),
                new_items=new_count,
                filtered_items=filtered_count,
                started_at=started_at,
            )

            self.sub_manager.mark_fetched(sub.id)

            # AI åˆ†æ (å¦‚æœå¯ç”¨ä¸”æœ‰æ–°å†…å®¹)
            if sub.ai_analysis_enabled and self.analyzer and new_count > 0:
                await self._run_analysis()

        except Exception as e:
            logger.error(f"é‡‡é›†è®¢é˜… {sub.name} å¤±è´¥: {e}")
            self.db.log_fetch(
                subscription_id=sub.id,
                source=sub.source,
                status="failed",
                error_message=str(e),
                started_at=started_at,
            )

    async def _fetch_twitter(self, sub) -> list[dict]:
        """æ‰§è¡Œ Twitter é‡‡é›† (å¸¦ credit è¿½è¸ª)"""
        items = []

        if sub.type == "keyword" or sub.type == "topic":
            if self.twitter_search:
                if not self._check_twitter_credit_budget(75):
                    logger.warning(f"Twitter credit é¢„ç®—ä¸è¶³ï¼Œè·³è¿‡è®¢é˜… {sub.target}")
                    return items
                sort = "Top"
                if sub.filters and sub.filters.get("sort"):
                    sort = sub.filters["sort"]
                items = await self.twitter_search.search(
                    query=sub.target,
                    limit=20,
                    sort=sort,
                )
                self._track_twitter_credits(75)
            else:
                logger.warning("TwitterAPI.io æœªé…ç½®ï¼Œæ— æ³•æ‰§è¡Œå…³é”®è¯æœç´¢")

        elif sub.type == "author":
            username = sub.target.lstrip("@")

            rss_items = await self.rss.get_author_content(
                author_id=username, platform="twitter"
            )
            if rss_items:
                items = rss_items
            elif self.twitter_search:
                if not self._check_twitter_credit_budget(75):
                    logger.warning(f"Twitter credit é¢„ç®—ä¸è¶³ï¼Œè·³è¿‡åšä¸» {username}")
                    return items
                items = await self.twitter_search.get_author_content(
                    author_id=username, limit=20
                )
                self._track_twitter_credits(75)

        return items

    async def _fetch_youtube(self, sub) -> list[dict]:
        """æ‰§è¡Œ YouTube é‡‡é›†"""
        items = []

        if sub.type == "keyword" or sub.type == "topic":
            if self.youtube:
                order = "relevance"
                if sub.filters and sub.filters.get("order"):
                    order = sub.filters["order"]
                items = await self.youtube.search(
                    query=sub.target,
                    limit=20,
                    order=order,
                )
            elif self.youtube_transcript:
                items = await self.youtube_transcript.search(
                    query=sub.target,
                    limit=20,
                )
            else:
                logger.warning("YouTube æ•°æ®æºå‡æœªé…ç½®")

        elif sub.type == "author":
            channel_id = sub.target

            if self.youtube:
                items = await self.youtube.get_author_content(
                    author_id=channel_id,
                    limit=20,
                )
            elif self.youtube_transcript:
                items = await self.youtube_transcript.get_author_content(
                    author_id=channel_id,
                    limit=20,
                )
            else:
                rss_items = await self.rss.get_author_content(
                    author_id=channel_id, platform="youtube"
                )
                if rss_items:
                    items = rss_items

        # ä¸ºé«˜è´¨é‡è§†é¢‘è·å–å­—å¹•
        if items and self.youtube_transcript:
            await self._enrich_youtube_transcripts(items)

        return items

    async def _enrich_youtube_transcripts(self, items: list[dict]) -> None:
        """ä¸ºé«˜è´¨é‡ YouTube è§†é¢‘è·å–å­—å¹•"""
        if not self.youtube_transcript:
            return

        for item in items[:5]:
            views = item.get("metrics", {}).get("views", 0)
            likes = item.get("metrics", {}).get("likes", 0)

            if views > 1000 or likes > 50:
                video_id = item.get("content_id")
                if video_id:
                    try:
                        transcript = await self.youtube_transcript.get_transcript(video_id)
                        if transcript:
                            item["transcript"] = transcript
                            logger.debug(f"è·å–å­—å¹•æˆåŠŸ: {video_id} ({len(transcript)} chars)")
                    except Exception as e:
                        logger.debug(f"è·å–å­—å¹•å¤±è´¥: {video_id}: {e}")

    # ========== æ¢ç´¢æµ (Explore/Discover) ==========

    async def run_explore_cycle(self) -> None:
        """æ‰§è¡Œæ¢ç´¢æµé‡‡é›†

        ä¸¤éƒ¨åˆ†:
        1. Twitter è¶‹åŠ¿å‘ç° â€” æ‹‰å–çƒ­é—¨è¶‹åŠ¿ï¼Œç”¨ Top å…³é”®è¯æœç´¢é«˜è´¨é‡å†…å®¹
        2. YouTube çƒ­é—¨å‘ç° â€” æ‹‰å–å„åœ°åŒºçƒ­é—¨è§†é¢‘
        3. ç”¨æˆ·è‡ªå®šä¹‰æ¢ç´¢å…³é”®è¯
        """
        if not self.dynamic_explore_enabled:
            logger.debug("æ¢ç´¢æµæœªå¯ç”¨")
            return

        logger.info("å¼€å§‹æ¢ç´¢æµé‡‡é›†...")
        total_new = 0

        # 1. Twitter è¶‹åŠ¿
        total_new += await self._explore_twitter_trends()

        # 2. YouTube çƒ­é—¨
        total_new += await self._explore_youtube_trending()

        # 3. ç”¨æˆ·è‡ªå®šä¹‰æ¢ç´¢å…³é”®è¯
        total_new += await self._explore_custom_keywords()

        logger.info(f"æ¢ç´¢æµé‡‡é›†å®Œæˆ: æ–°å¢ {total_new} æ¡å†…å®¹")

    async def _explore_twitter_trends(self) -> int:
        """Twitter è¶‹åŠ¿æ¢ç´¢ (å¸¦ credit é¢„ç®—æ§åˆ¶)"""
        if not self.twitter_search:
            return 0

        woeids = [
            int(w.strip())
            for w in self.dynamic_explore_twitter_woeids.split(",")
            if w.strip()
        ]
        max_trends = self.dynamic_max_trends_per_woeid
        search_limit = self.dynamic_max_search_per_keyword

        # é¢„ä¼° credit: æ¯ä¸ª WOEID è¶‹åŠ¿ ~450 + æ¯æ¬¡æœç´¢ ~75
        estimated = len(woeids) * 450 + len(woeids) * max_trends * 75
        if not self._check_twitter_credit_budget(estimated):
            logger.warning(f"Twitter è¶‹åŠ¿æ¢ç´¢: credit é¢„ç®—ä¸è¶³ï¼Œè·³è¿‡ (é¢„ä¼° {estimated})")
            return 0

        new_total = 0
        for woeid in woeids:
            try:
                trends = await self.twitter_search.get_trends(woeid=woeid, count=10)
                self._track_twitter_credits(450)
                if not trends:
                    continue

                for trend in trends[:max_trends]:
                    if not self._check_twitter_credit_budget(75):
                        logger.warning("Twitter credit é¢„ç®—è€—å°½ï¼Œåœæ­¢è¶‹åŠ¿æœç´¢")
                        break

                    query = trend.get("query") or trend.get("name")
                    if not query:
                        continue

                    items = await self.twitter_search.search(
                        query=query, limit=search_limit, sort="Top"
                    )
                    self._track_twitter_credits(75)
                    if not items:
                        continue

                    for item in items:
                        item["subscription_id"] = None

                    if self.smart_filter:
                        items = self.smart_filter.filter_batch(items)
                    else:
                        for item in items:
                            item["quality_score"] = self._calc_quality_score(item)
                        items = [i for i in items if i.get("quality_score", 0) >= self.dynamic_min_quality_score]

                    if items:
                        new_count, _ = self.db.save_contents_batch(items)
                        new_total += new_count

            except Exception as e:
                logger.error(f"Twitter è¶‹åŠ¿æ¢ç´¢å¤±è´¥ (woeid={woeid}): {e}")

        if new_total > 0:
            logger.info(f"Twitter è¶‹åŠ¿æ¢ç´¢: æ–°å¢ {new_total} æ¡ (credit å·²ç”¨: {self._twitter_credits_used})")
        return new_total

    async def _explore_youtube_trending(self) -> int:
        """YouTube çƒ­é—¨è§†é¢‘æ¢ç´¢"""
        if not self.youtube:
            return 0

        new_total = 0
        regions = [
            r.strip()
            for r in self.dynamic_explore_youtube_regions.split(",")
            if r.strip()
        ]
        category = settings.explore_youtube_category  # category ä¸å¸¸æ”¹ï¼Œä¿æŒ .env

        for region in regions:
            try:
                items = await self.youtube.get_trending(
                    region_code=region,
                    category_id=category,
                    limit=10,
                )
                if not items:
                    continue

                for item in items:
                    item["subscription_id"] = None

                if self.smart_filter:
                    items = self.smart_filter.filter_batch(items)
                else:
                    for item in items:
                        item["quality_score"] = self._calc_quality_score(item)
                    items = [i for i in items if i.get("quality_score", 0) >= self.dynamic_min_quality_score]

                if items:
                    new_count, _ = self.db.save_contents_batch(items)
                    new_total += new_count

                # ä¸ºçƒ­é—¨è§†é¢‘è·å–å­—å¹•
                if items and self.youtube_transcript:
                    await self._enrich_youtube_transcripts(items)

            except Exception as e:
                logger.error(f"YouTube çƒ­é—¨æ¢ç´¢å¤±è´¥ (region={region}): {e}")

        if new_total > 0:
            logger.info(f"YouTube çƒ­é—¨æ¢ç´¢: æ–°å¢ {new_total} æ¡")
        return new_total

    async def _explore_custom_keywords(self) -> int:
        """ç”¨æˆ·è‡ªå®šä¹‰æ¢ç´¢å…³é”®è¯ (å¸¦ credit é¢„ç®—æ§åˆ¶)"""
        keywords = [
            k.strip()
            for k in self.dynamic_explore_keywords.split(",")
            if k.strip()
        ]
        if not keywords:
            return 0

        search_limit = self.dynamic_max_search_per_keyword
        new_total = 0

        for keyword in keywords:
            # Twitter æœç´¢ (å¸¦ credit æ£€æŸ¥)
            if self.twitter_search and self._check_twitter_credit_budget(75):
                try:
                    items = await self.twitter_search.search(
                        query=keyword, limit=search_limit, sort="Top"
                    )
                    self._track_twitter_credits(75)
                    for item in items:
                        item["subscription_id"] = None
                    if self.smart_filter:
                        items = self.smart_filter.filter_batch(items)
                    if items:
                        new_count, _ = self.db.save_contents_batch(items)
                        new_total += new_count
                except Exception as e:
                    logger.error(f"æ¢ç´¢å…³é”®è¯ Twitter æœç´¢å¤±è´¥ ({keyword}): {e}")

            # YouTube æœç´¢ (viewCount æ’åºè·å–çƒ­é—¨)
            if self.youtube:
                try:
                    items = await self.youtube.search(
                        query=keyword, limit=search_limit, order="viewCount"
                    )
                    for item in items:
                        item["subscription_id"] = None
                    if self.smart_filter:
                        items = self.smart_filter.filter_batch(items)
                    if items:
                        new_count, _ = self.db.save_contents_batch(items)
                        new_total += new_count
                except Exception as e:
                    logger.error(f"æ¢ç´¢å…³é”®è¯ YouTube æœç´¢å¤±è´¥ ({keyword}): {e}")

        if new_total > 0:
            logger.info(f"è‡ªå®šä¹‰æ¢ç´¢å…³é”®è¯: æ–°å¢ {new_total} æ¡")
        return new_total

    # ========== æ¨é€è°ƒåº¦ (ä¸æŠ“å–è§£è€¦) ==========

    async def run_notify_batch(self) -> None:
        """å®šæ—¶æ¨é€ä»»åŠ¡

        ä»æ•°æ®åº“å–æœªé€šçŸ¥çš„é«˜è´¨é‡å†…å®¹ï¼Œæ‰¹é‡æ¨é€åˆ°é£ä¹¦ã€‚
        ä¸æŠ“å–å®Œå…¨è§£è€¦ï¼ŒæŒ‰ notify_schedule é…ç½®çš„æ—¶é—´ç‚¹è¿è¡Œã€‚
        """
        # åŠ¨æ€æ£€æŸ¥æ•°æ®åº“ä¸­æ˜¯å¦æœ‰æ›´æ–°çš„é£ä¹¦ webhook é…ç½®
        self._refresh_feishu_client()

        if not self.feishu:
            return

        try:
            threshold = self.dynamic_min_quality_score
            unnotified = self.db.get_unnotified_contents(
                limit=settings.max_notify_per_batch,
                min_quality=threshold,
            )
            if not unnotified:
                logger.debug("æ— å¾…æ¨é€å†…å®¹")
                return

            logger.info(f"å¼€å§‹æ‰¹é‡æ¨é€ {len(unnotified)} æ¡å†…å®¹...")

            success_count = 0
            for content in unnotified:
                try:
                    sub_name = None
                    if content.subscription_id:
                        sub = self.sub_manager.get(content.subscription_id)
                        if sub:
                            sub_name = sub.name

                    msg = MessageBuilder.build_content_notification(
                        source=content.source,
                        title=content.title,
                        content=content.content or "",
                        author=content.author or "unknown",
                        url=content.url or "",
                        metrics=content.metrics,
                        ai_analysis=content.ai_analysis,
                        subscription_name=sub_name,
                    )

                    source_emoji = {"twitter": "ğŸ¦", "youtube": "ğŸ“º"}.get(
                        content.source, "ğŸ“°"
                    )
                    title = f"{source_emoji} InfoHunter æ–°å†…å®¹"

                    success = await self.feishu.send_markdown_card(title, msg)
                    if success:
                        self.db.mark_contents_notified([content.id])
                        success_count += 1

                except Exception as e:
                    logger.error(f"æ¨é€å¤±è´¥ (content_id={content.content_id}): {e}")

            logger.info(f"æ‰¹é‡æ¨é€å®Œæˆ: {success_count}/{len(unnotified)} æˆåŠŸ")

        except Exception as e:
            logger.error(f"æ‰¹é‡æ¨é€ä»»åŠ¡å¤±è´¥: {e}")

    # ========== AI åˆ†æ ==========

    async def _run_analysis(self) -> None:
        """è¿è¡Œ AI åˆ†æ"""
        if not self.analyzer:
            return

        try:
            unanalyzed = self.db.get_unanalyzed_contents(limit=10)
            if not unanalyzed:
                return

            logger.info(f"å¼€å§‹ AI åˆ†æ {len(unanalyzed)} æ¡å†…å®¹...")

            for content in unanalyzed:
                try:
                    result = await self.analyzer.analyze_content(
                        content=content.content or "",
                        source=content.source,
                        title=content.title,
                        author=content.author,
                        metrics=content.metrics,
                        transcript=content.transcript,
                    )

                    if result["status"] == "success" and result["analysis"]:
                        self.db.update_ai_analysis(content.id, result["analysis"])

                        analysis = result["analysis"]
                        if isinstance(analysis, dict) and analysis.get("importance"):
                            relevance = analysis["importance"] / 10.0
                            self.db.update_scores(
                                content.id, relevance_score=relevance
                            )

                except Exception as e:
                    logger.error(f"åˆ†æå†…å®¹ {content.content_id} å¤±è´¥: {e}")

        except Exception as e:
            logger.error(f"AI åˆ†æä»»åŠ¡å¤±è´¥: {e}")

    # ========== æŠ¥å‘Š ==========

    async def send_daily_report(self) -> None:
        """å‘é€æ—¥æŠ¥ (AI Newsletter æ‘˜è¦)"""
        if not self.feishu:
            return

        try:
            now = datetime.now(self.SERVER_TZ)
            since = now - timedelta(hours=24)
            since_naive = since.replace(tzinfo=None)

            contents = self.db.get_contents_for_report(since=since_naive)
            if not contents:
                logger.info("è¿‡å» 24 å°æ—¶æ— å†…å®¹ï¼Œè·³è¿‡æ—¥æŠ¥")
                return

            # AI è¶‹åŠ¿åˆ†æ (è¶‹åŠ¿é›·è¾¾ + Newsletter æ‘˜è¦)
            ai_summary = None
            if self.analyzer:
                items_for_analysis = [
                    {
                        "content": c.content or "",
                        "title": c.title,
                        "source": c.source,
                        "author": c.author or "",
                        "metrics": c.metrics,
                    }
                    for c in contents[:30]
                    if c.content
                ]
                if items_for_analysis:
                    result = await self.analyzer.analyze_batch(
                        items_for_analysis, focus="daily_newsletter"
                    )
                    if result["status"] == "success":
                        ai_summary = result["analysis"]

            contents_data = [
                {
                    "source": c.source,
                    "title": c.title,
                    "content": c.content or "",
                    "author": c.author or "",
                    "url": c.url or "",
                }
                for c in contents
            ]

            msg = MessageBuilder.build_daily_report(
                contents_data, date=now, ai_summary=ai_summary
            )

            success = await self.feishu.send_markdown_card("ğŸ“Š InfoHunter æ—¥æŠ¥", msg)
            if success:
                logger.info(f"æ—¥æŠ¥æ¨é€æˆåŠŸï¼Œå…± {len(contents)} æ¡å†…å®¹")
            else:
                logger.error("æ—¥æŠ¥æ¨é€å¤±è´¥")

        except Exception as e:
            logger.error(f"å‘é€æ—¥æŠ¥å¤±è´¥: {e}")

    async def send_weekly_report(self) -> None:
        """å‘é€å‘¨æŠ¥"""
        if not self.feishu:
            return

        try:
            now = datetime.now(self.SERVER_TZ)
            since = now - timedelta(days=7)
            since_naive = since.replace(tzinfo=None)

            contents = self.db.get_contents_for_report(since=since_naive, limit=500)
            if not contents:
                logger.info("è¿‡å» 7 å¤©æ— å†…å®¹ï¼Œè·³è¿‡å‘¨æŠ¥")
                return

            ai_summary = None
            if self.analyzer:
                items_for_analysis = [
                    {
                        "content": c.content or "",
                        "title": c.title,
                        "source": c.source,
                        "author": c.author or "",
                        "metrics": c.metrics,
                    }
                    for c in contents[:50]
                    if c.content
                ]
                if items_for_analysis:
                    result = await self.analyzer.analyze_batch(
                        items_for_analysis, focus="weekly_summary"
                    )
                    if result["status"] == "success":
                        ai_summary = result["analysis"]

            contents_data = [
                {
                    "source": c.source,
                    "title": c.title,
                    "content": c.content or "",
                    "author": c.author or "",
                    "url": c.url or "",
                }
                for c in contents
            ]

            msg = MessageBuilder.build_weekly_report(
                contents_data,
                week_start=since_naive,
                week_end=now.replace(tzinfo=None),
                ai_summary=ai_summary,
            )

            success = await self.feishu.send_markdown_card("ğŸ“Š InfoHunter å‘¨æŠ¥", msg)
            if success:
                logger.info(f"å‘¨æŠ¥æ¨é€æˆåŠŸï¼Œå…± {len(contents)} æ¡å†…å®¹")

        except Exception as e:
            logger.error(f"å‘é€å‘¨æŠ¥å¤±è´¥: {e}")

    # ========== è´¨é‡è¯„åˆ† ==========

    def _calc_quality_score(self, item: dict) -> float:
        """è®¡ç®—å†…å®¹è´¨é‡è¯„åˆ† (0-1)"""
        score = 0.0
        metrics = item.get("metrics", {})

        likes = metrics.get("likes", 0)
        retweets = metrics.get("retweets", 0)
        views = metrics.get("views", 0)
        replies = metrics.get("replies", 0)

        engagement = likes + retweets * 2 + replies * 3
        if engagement > 1000:
            score += 0.5
        elif engagement > 100:
            score += 0.3
        elif engagement > 10:
            score += 0.15
        elif engagement > 0:
            score += 0.05

        content = item.get("content", "")
        if len(content) > 200:
            score += 0.2
        elif len(content) > 50:
            score += 0.1
        elif len(content) > 10:
            score += 0.05

        if item.get("title"):
            score += 0.1

        if item.get("media_attachments"):
            score += 0.1

        if views > 100000:
            score += 0.1
        elif views > 10000:
            score += 0.05

        return min(score, 1.0)

    # ========== è°ƒåº¦å¾ªç¯ ==========

    async def run_fetch_cycle(self) -> None:
        """æ‰§è¡Œä¸€è½®è®¢é˜…æµé‡‡é›†"""
        due_subs = self.sub_manager.get_due_subscriptions()
        if not due_subs:
            logger.debug("æ— éœ€é‡‡é›†çš„è®¢é˜…")
            return

        logger.info(f"æœ¬è½®éœ€é‡‡é›† {len(due_subs)} ä¸ªè®¢é˜…")
        for sub in due_subs:
            await self.fetch_subscription(sub)

        if self.smart_filter:
            self.smart_filter.reset_seen_hashes()

    async def start(self) -> None:
        """å¯åŠ¨ InfoHunter"""
        await self.init()
        self.running = True

        now = get_local_time()
        logger.info(f"InfoHunter å¯åŠ¨ ({now.strftime('%Y-%m-%d %H:%M')} {settings.timezone})")

        # 1. è®¢é˜…æµé‡‡é›†è°ƒåº¦ (æ¯ 5 åˆ†é’Ÿæ£€æŸ¥)
        self.scheduler.add_job(
            self.run_fetch_cycle,
            trigger=IntervalTrigger(minutes=5),
            id="fetch_cycle",
            name="è®¢é˜…æµé‡‡é›†",
            replace_existing=True,
        )

        # 2. æ¢ç´¢æµé‡‡é›†è°ƒåº¦
        if self.dynamic_explore_enabled:
            explore_minutes = max(self.dynamic_explore_interval // 60, 30)
            self.scheduler.add_job(
                self.run_explore_cycle,
                trigger=IntervalTrigger(minutes=explore_minutes),
                id="explore_cycle",
                name="æ¢ç´¢æµé‡‡é›†",
                replace_existing=True,
            )
            logger.info(f"æ¢ç´¢æµå·²å¯ç”¨: æ¯ {explore_minutes} åˆ†é’Ÿé‡‡é›†ä¸€æ¬¡")

        # 3. æ¨é€è°ƒåº¦ (æŒ‰å›ºå®šæ—¶é—´ç‚¹)
        notify_times = [
            t.strip()
            for t in self.dynamic_notify_schedule.split(",")
            if t.strip()
        ]
        for i, time_str in enumerate(notify_times):
            try:
                hour, minute = time_str.split(":")
                self.scheduler.add_job(
                    self.run_notify_batch,
                    trigger=CronTrigger(
                        hour=int(hour), minute=int(minute), timezone=self.SERVER_TZ
                    ),
                    id=f"notify_batch_{i}",
                    name=f"å®šæ—¶æ¨é€ ({time_str})",
                    replace_existing=True,
                )
            except ValueError:
                logger.warning(f"æ— æ•ˆçš„æ¨é€æ—¶é—´æ ¼å¼: {time_str}")
        if notify_times:
            logger.info(f"æ¨é€è°ƒåº¦å·²é…ç½®: {', '.join(notify_times)}")

        # 4. æ—¥æŠ¥ (æ¯å¤© 9:00)
        self.scheduler.add_job(
            self.send_daily_report,
            trigger=CronTrigger(hour=9, minute=0, timezone=self.SERVER_TZ),
            id="daily_report",
            name="æ—¥æŠ¥æ¨é€",
            replace_existing=True,
        )

        # 5. å‘¨æŠ¥ (æ¯å‘¨ä¸€ 9:30)
        self.scheduler.add_job(
            self.send_weekly_report,
            trigger=CronTrigger(
                day_of_week=0, hour=9, minute=30, timezone=self.SERVER_TZ
            ),
            id="weekly_report",
            name="å‘¨æŠ¥æ¨é€",
            replace_existing=True,
        )

        self.scheduler.start()

        # é¦–æ¬¡é‡‡é›† (ä»…è®¢é˜…æµï¼Œæ¢ç´¢æµç­‰å¾…è°ƒåº¦å™¨è§¦å‘ï¼Œé¿å…å¯åŠ¨æ—¶æ¶ˆè€—å¤§é‡ credit)
        logger.info("æ‰§è¡Œé¦–æ¬¡è®¢é˜…æµé‡‡é›†...")
        await self.run_fetch_cycle()
        self.is_first_run = False
        logger.info("æ¢ç´¢æµå°†åœ¨ä¸‹ä¸€ä¸ªè°ƒåº¦å‘¨æœŸè‡ªåŠ¨æ‰§è¡Œ (ä¸åœ¨å¯åŠ¨æ—¶ç«‹å³æ‰§è¡Œä»¥èŠ‚çœ credit)")

        # ä¿æŒè¿è¡Œ
        try:
            while self.running:
                await asyncio.sleep(1)
        except asyncio.CancelledError:
            logger.info("æ”¶åˆ°åœæ­¢ä¿¡å·")

    async def stop(self) -> None:
        """ä¼˜é›…åœæ­¢ InfoHunter"""
        if not self.running:
            return
        logger.info("æ­£åœ¨åœæ­¢ InfoHunter...")
        self.running = False
        if self.scheduler and self.scheduler.running:
            self.scheduler.shutdown(wait=True)
            logger.info("APScheduler å·²åœæ­¢")
        logger.info("InfoHunter å·²åœæ­¢")


async def main():
    """ä¸»å‡½æ•°"""
    hunter = InfoHunter()

    loop = asyncio.get_event_loop()

    def signal_handler():
        logger.info("æ”¶åˆ°ç»ˆæ­¢ä¿¡å·...")
        asyncio.create_task(hunter.stop())

    for sig in (signal.SIGINT, signal.SIGTERM):
        loop.add_signal_handler(sig, signal_handler)

    try:
        await hunter.start()
    except Exception as e:
        logger.error(f"InfoHunter å¼‚å¸¸: {e}")
    finally:
        await hunter.stop()


if __name__ == "__main__":
    asyncio.run(main())
